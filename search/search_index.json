{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the CLIMB-BIG-DATA documentation!</p> <p>These docs are designed to help you get the best out of the CLIMB-BIG-DATA infrastructure.</p> <p>Bryn is the web interface for CLIMB-BIG-DATA, and is the primary way to access the infrastructure. From Bryn, you can manage your team, its resources, the S3 buckets and your notebooks. CLIMB Notebooks, on the other hand, are Jupyter notebooks fully integrated with CLIMB and its storage, with first-class support to Nextflow pipelines.</p> <p></p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Registration How to register and access CLIMB-BIG-DATA.</p> <p>Authentication How to login to Bryn, and setup two-factor authentication.</p>"},{"location":"#notebook-servers","title":"Notebook Servers","text":"<p>Everything you need to understand, use and get the most out of Jupyter Notebook Servers.</p> <p>Read this first! An introduction to the what and why of notebook servers.</p> <p>Quick start How to launch, access and get started using a notebook server.</p> <p>Using the terminal How to use the terminal inside a notebook server, with an explanation of caveats.</p> <p>Using the interface Explore various features of the notebook server interface.</p> <p>Using Jupyter notebooks The basics of working with Jupyter notebooks within a notebook server.</p> <p>Using RStudio How to get started with RStudio within a notebook server.</p> <p>Understanding storage An explanation of the different storage options available and when to use what.</p> <p>Installing software with Conda How to install software using Conda, in the context of a containerized environment.</p> <p>Using Nextflow How to use Nextflow with CLIMB-BIG-DATA.</p> <p>Using Visual Studio Code How to connect to your CLIMB Notebook and work from Visual Studio Code.</p> <p>403 Forbidden Error An explanation of how to resolve login error 403 when accessing notebooks.</p>"},{"location":"#walkthroughs","title":"Walkthroughs","text":"<p>Metagenomics walkthrough A simple walk-through of some CLIMB-BIG-DATA functionality.</p> <p>Genome assembly Assembling a genome from short reads using SPAdes.</p> <p>Custom Nextflow workflows A guide on writing custom nextflow workflows.</p> <p>QIIME 2 How to install QIIME 2 on a notebook server and basic usage.</p> <p>nf-core pipelines How to run some of the nf-core pipelines on CLIMB notebooks.</p>"},{"location":"getting-started/authentication/","title":"Authentication","text":""},{"location":"getting-started/authentication/#where-do-i-sign-in","title":"Where do I sign in?","text":"<p>All CLIMB-BIG-DATA resources are accessed via the Bryn web interface.</p> <p>You can login to Bryn here.</p>"},{"location":"getting-started/authentication/#two-factor-authentication","title":"Two-factor authentication","text":"<p>Two-factor authentication is mandatory, and users will be required to set this up on first login. This means that a code will be required from an authenticator app on a mobile or desktop device, in addition to your password.</p> <p>For convenience, if using the same browser and device, you will only be required to enter this every 30 days.</p>"},{"location":"getting-started/authentication/#initial-setup","title":"Initial setup","text":"<p>When you first login to your account, authentication will need to be set up. You will be presented with the following interface:</p> <p></p> <p>Hit enable and proceed to follow the instructions. You will be prompted to install an authenticator app on your mobile device. Our current recommendations (in order) are:</p> <ul> <li>Authy (Desktop app available)</li> <li>Microsoft Authenticator</li> <li>Google Authenticator</li> </ul> <p>Info</p> <p>Authy and Microsoft Authenticator make it easier to backup and recover your codes in the case of loss or change of device, whereas Google requires a manual export/import.</p> <p>Please do enable backups for your app at this stage, before you forget!</p>"},{"location":"getting-started/authentication/#authy-backups","title":"Authy backups","text":"<p>Authy has a backup feature to enable recovery in case you lose or replace your phone. Your data is encrypted and only decrypted on the devices using a password that only you will know.</p> <p>See the Authy documentation for further details.</p>"},{"location":"getting-started/authentication/#microsoft-authenticator-backups","title":"Microsoft Authenticator backups","text":"<p>See the Microsoft documentation for details on how to enable this.</p>"},{"location":"getting-started/authentication/#scanning-the-qr-code","title":"Scanning the QR code","text":"<p>In all authenticator apps, the default way to add an app is to scan a QR code. Bryn will present the code for you to scan, and ask you to enter the generated token before it expires (every 30s). If it does expire while you are typing, just enter the new one.</p> <p>Congratulations, two-factor authentication is now enabled.</p>"},{"location":"getting-started/authentication/#backup-codes","title":"Backup codes","text":"<p>You will now have the option to generate backup codes. This is a list of single-use codes that enable you to gain access in the event of the loss of your device. This is strongly recommended.</p> <p>Follow the link to 'setup your backup codes' as seen in the screenshot below:</p> <p></p> <p>Next, hit the 'Generate tokens' button:</p> <p></p> <p>Print, save or otherwise make a note of these codes and keep them in a secure place.</p> <p>You can now proceed to the Bryn dashboard.</p>"},{"location":"getting-started/authentication/#logging-in-to-bryn-after-2fa-is-set-up","title":"Logging in to Bryn after 2FA is set up","text":"<p>When you next login to Bryn, after entering your username and password, a token will be requested from your 2FA device:</p> <p></p> <p>If you wish, mark the check box \"Don't ask again on this device for 30 days\".</p>"},{"location":"getting-started/authentication/#using-one-of-your-backup-tokens","title":"Using one of your backup tokens","text":"<p>If you don't have your device to hand, or you've lost it, you can use a single-use backup token.</p> <p>To use a backup token, follow the link here:</p> <p></p>"},{"location":"getting-started/authentication/#managing-two-factor-authentication","title":"Managing two-factor authentication","text":"<p>When logged in to the Bryn dashboard, navigate to 'User profile' under the 'Team management' menu.</p> <p></p> <p>Here, you can view or generate your backup codes and disable 2FA (for example you wish to use a different device or app).</p> <p></p>"},{"location":"getting-started/authentication/#disable-your-current-2fa-device-switch-device-or-app","title":"Disable your current 2FA device (switch device or app)","text":"<p>If you wish to change your device or app, you'll need to disable your current two-factor authentication. This will log you out and restart the process.</p> <p>Please note: if you are only changing device, the best option is to use the backup/recovery method for your authenticator app.</p> <p>If you are sure you'd like to start again, hit the 'Disable Two-Factor Authentication' button.</p> <p></p> <p>Confirm you are sure, and you will be logged out. Log back in to restart the 2FA setup process with your new app or device.</p>"},{"location":"getting-started/authentication/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"getting-started/authentication/#i-can-no-longer-access-my-2fa-device-or-phone-and-i-cant-recover-from-a-backup","title":"I can no longer access my 2FA device or phone, and I can't recover from a backup","text":"<p>If you are still logged in to Bryn, follow the steps in 2FA Management.</p> <p>Otherwise, please contact our support team at support@climb.ac.uk.</p>"},{"location":"getting-started/authentication/#why-are-my-authenticator-codes-not-working","title":"Why are my authenticator codes not working?","text":"<p>If you have setup your authentication but the 6-digit code does not work, you may need to check that the time on your device is set to auto-sync. Without this setting, the time on the device and server will not match and the codes will not work. This may be a term such as: \"Use network provided time/time-zone\".</p>"},{"location":"getting-started/authentication/#why-are-my-backup-codes-not-working","title":"Why are my backup codes not working?","text":"<p>You will only have X backup codes to use. Once they have been used up, you will not be able to access your account. These are to be used when either the app/desktop authenticators do not work. You can check your codes and how many you have left, please see 2FA Management.</p>"},{"location":"getting-started/authentication/#i-have-the-authenticator-app-why-wont-it-let-me-scan-anything","title":"I have the authenticator app, why won't it let me scan anything?","text":"<p>If you are not prompted to scan the QR code, you may need to change the permissions for the app to access your camera.</p>"},{"location":"getting-started/authentication/#i-dont-have-a-smart-phone-how-do-i-login","title":"I don't have a smart phone, how do I login?","text":"<p>There are desktop versions of authenticators that you can use, such as Authy.</p>"},{"location":"getting-started/how-to-register/","title":"How to register","text":""},{"location":"getting-started/how-to-register/#who-can-register","title":"Who can register","text":"<p>We divide users into three categories:</p> <ul> <li>Primary users: Those with salaried positions in UK academic institutions, government agencies or healthcare systems who have the status of independent researchers and/or team leaders.</li> <li>Secondary users: Those working under the direction of primary users who include students, post-doctoral researchers and overseas collaborators.</li> <li>Industrial users: Users in industry should contact us to discuss terms and conditions for industrial users.</li> </ul> <p>Primary users should head over to the Bryn registration page to get started.</p> <p>To add secondary users to a team, please see inviting users to your team.</p> <p>Warning</p> <p>Please note: only primary users should register to create a new team</p>"},{"location":"getting-started/how-to-register/#primary-user-registration","title":"Primary user registration","text":"<p>When you head over to the Bryn registration page, you should see the registration form:</p> <p></p> <p>Accept the terms if you are happy, and you will then be asked information regarding your \u201cPrimary user details\u201d including contact information and your position. You will also be asked about your \u201cTeam details\u201d for your CLIMB-BIG-DATA team account. This includes information on where you currently work and why you would like to use CLIMB's resources.</p> <p>The registration request will be reviewed by a member of our management team. Please provide as much information as possible about your role and research to speed up the process. If we do not feel enough information has been provided, we may contact you. If your registration is successful, you will receive a verification email. Following verification, you will be taken to the Bryn portal.</p> <p>Warning</p> <p>Please be aware that primary users will be required to renew a group license every 3 months. For paying users, the length of a valid license period will be extended. If this license is not renewed, your access to resources will be blocked. You will be sent an email reminder for this.</p>"},{"location":"notebook-servers/","title":"Notebook Servers","text":"<p>Everything you need to understand, use and get the most out of Jupyter Notebook Servers.</p> <p>Read this first! An introduction to the what and why of notebook servers.</p> <p>Quick start How to launch, access and get started using a notebook server.</p> <p>Using the terminal How to use the terminal inside a notebook server, with an explanation of caveats.</p> <p>Using the interface Explore various features of the notebook server interface.</p> <p>Using Jupyter notebooks The basics of working with Jupyter notebooks within a notebook server.</p> <p>Using RStudio How to get started with RStudio within a notebook server.</p> <p>Understanding storage An explanation of the different storage options available and when to use what.</p> <p>Installing software with Conda How to install software using Conda, in the context of a containerized environment.</p> <p>Using Nextflow How to use Nextflow with CLIMB-BIG-DATA.</p> <p>Using Visual Studio Code How to connect to your CLIMB Notebook and work from Visual Studio Code.</p> <p>403 Forbidden Error An explanation of how to resolve login error 403 when accessing notebooks.</p>"},{"location":"notebook-servers/403-forbidden-error/","title":"403 Forbidden Error","text":"<p>This issue commonly arises during transitions between teams within BRYN, especially when users are managing multiple tasks across various teams. It typically occurs when users have a notebook already open in one team and then proceed to switch to another team, attempting to open a new notebook.</p> <p>Addressing this issue is straightforward with the following steps:</p> <p>1.) Click either 'Home' or 'Token':</p> <p></p> <p>2.) Click 'Logout':</p> <p></p> <p>3.) Once you have successfully logged out, click 'Login':</p> <p></p> <p>4.) You will then be taken to the BRYN login page. Click 'Authorize':</p> <p></p> <p>5.) You will then be able to access the notebook. Enjoy!</p>"},{"location":"notebook-servers/installing-software-with-conda/","title":"Installing software with Conda","text":"<p>It's critical to understand how Conda works with notebook servers, and the caveats compared to using it locally or on a VM.</p> <p>Firstly, understand that you cannot install new software to the base Conda environment. Let's have a look at why.</p> <pre><code>$ conda info --envs\nbase                     /opt/conda\n</code></pre> <p>The base Conda environment is installed at <code>/opt/conda</code>. Since we are running inside a container, any changes made to this part of the filesystem will not be retained once the container is stopped and restarted (unlike your home dir and shares, which are persisted).</p> <p>We've made the base environment read only to prevent any confusion.</p> <p>Tip</p> <p>You must create a new Conda environment before installing software!</p>"},{"location":"notebook-servers/installing-software-with-conda/#default-condarc","title":"Default .condarc","text":"<p>When you first launch a notebook server, we generate a default <code>.condarc</code> in your home directory. This sets the path for your new environments to <code>/shared/team/conda/$JUPYTER_USERNAME</code>. Why? As mentioned above, your home directory is relatively small compared with your team share, so it makes sense to use the larger mount. In addition, it becomes easy to share Conda environments with other team members.</p> <pre><code>jovyan:~$ cat ~/.condarc\nenvs_dirs:\n  - /shared/team/conda/demouser.andy-bryn-dev-t\n[...]\n</code></pre>"},{"location":"notebook-servers/installing-software-with-conda/#creating-new-conda-environments","title":"Creating new Conda environments","text":"<p>Understanding the above, you can create new Conda environments in the usual way. The only caveat is that if you wish to use this environment with Jupyter notebooks, you must install <code>ipykernel</code>.</p> <p>Warning</p> <p>If you don't install <code>ipykernel</code> in a new Conda environment, it won't show up on the launcher or be available to select within the python notebooks interface. However, you can use the environment just fine within a terminal.</p> <p>Let's go ahead and install bactopia as an example.</p> <p>If you try listing channels, you'll see you already have <code>conda-forge</code> and <code>bioconda</code> set:</p> <pre><code>jovyan:~$ conda config --show channels\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\n</code></pre> <p>Now, lets install <code>bactopia</code>:</p> <pre><code>jovyan:~$ conda create -y -n bactopia bactopia ipykernel\n...grab a coffee...\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate bactopia\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre> <p>Done. Listing your environments again to confirm the location:</p> <pre><code>jovyan:~$ conda info --envs\n# conda environments:\n#\nbase                     /opt/conda\nbactopia                 /shared/team/conda/demouser.andy-bryn-dev-t/bactopia\n</code></pre> <p>And finally let's activate the environment:</p> <pre><code>jovyan:~$ conda activate bactopia\n(bactopia) jovyan:~$ bactopia --version\nbactopia 2.2.0\n</code></pre>"},{"location":"notebook-servers/installing-software-with-conda/#using-conda-environments-in-jupyter-notebooks","title":"Using Conda environments in Jupyter notebooks","text":"<p>If you installed <code>ipykernel</code> when you created a Conda environment, as in the example above, then it should be available to use in your Jupyter notebooks. Near the top-right of any Jupyter notebook will be a text box with something like \"Python [conda env:root]\" or \"Python 3 (ipykernel)\", depending on which launcher icon you clicked on to start the notebook.  This specifies which Python kernel is being used.  To switch to you Conda environment, click on this text field and select the kernel associated with your Conda environment from the options under Start Preferred Kernel.  E.g., \"Python [conda env:bactopia]\" in the example above.</p> <p>After changing the kernel, it's worth restarting the kernel and running your code cells again.</p> <p>If the environment doesn't appear in the list of kernels, you may have not installed <code>ipykernel</code> when you created it, in which case you can install it later.  To do so, open a new terminal, activate the environment and install <code>ipykernel</code> with e.g. <pre><code>jovyan:~$ conda activate bactopia\n(bactopia) jovyan:~$ conda install -y ipykernel\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n...\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(bactopia) jovyan:~$\n</code></pre> The kernel <code>Python [conda env:bactopia]</code> should now appear in the dropdown list, including in Jupyter notebooks that are already running.</p>"},{"location":"notebook-servers/quick-start/","title":"Notebook servers, quick start","text":""},{"location":"notebook-servers/quick-start/#how-to-launch-and-access-a-notebook-server","title":"How to launch and access a notebook server","text":"<ol> <li>Log in to Bryn.</li> <li>Using the navigation menu on the left hand side, select 'Notebook servers' under the 'Compute' subheading.</li> <li>Click the 'Launch notebook server' green action button on the right hand side.</li> <li>Select a profile, for example 'Standard server' or 'GPU server' (tier dependent).</li> <li>Click 'Launch Server' and monitor the progress bar.</li> <li>Once ready, click the URL beneath the 'User notebook server'.</li> <li>On first login, you may be asked to authorize access to your Bryn account. Click 'Authorize'.</li> <li>The JupyterLab interactive computing interface should open in a new tab.</li> </ol>"},{"location":"notebook-servers/quick-start/#finding-your-way-around-the-jupyterlab-interface","title":"Finding your way around the JupyterLab interface","text":"<p>We highly recommend spending a few moments familiarizing yourself with the basic JupyterLab interface. Head to the JupyterLab interface docs to get started.</p> <p>Fundamentally, your screen is divided into a few areas. You'll see context menus at the top (File, Edit, View, Run etc.), a file browser pane on the left, and an activity area that initially displays a launcher interface with tiles. Clicking on one of these tiles will open a new tab in the activity area.</p>"},{"location":"notebook-servers/quick-start/#what-now","title":"What now?","text":"<p>Since most users are familiar with using a terminal to access a system shell (just like on a VM!), let's start there.</p>"},{"location":"notebook-servers/read-this-first/","title":"Read this first - Notebook Servers","text":"<p>Welcome to CLIMB's Jupyter Notebook Servers, your new home on CLIMB-BIG-DATA. The Notebook server is a lightweight virtualised installation of Linux, capable of running complex data science and bioinformatics tasks.</p> <p>In this short introduction, we will introduce the notebook server and help you move in into your new home. The working pattern may be slightly different to what you are used to, so we encourage you to peruse this and associated documentation to get the most out of the Jupyter Notebook Servers.</p> <p>These notebooks are truly powerful if used correctly, as shown in these walkthroughs:</p> <ul> <li>Assembling a genome from short reads (e.g. Illumina) using SPAdes</li> <li>Metagenomics in Brum</li> </ul>"},{"location":"notebook-servers/read-this-first/#advantages-of-the-notebook","title":"Advantages of the Notebook","text":"<p>Just as if you have your own physical machine, you'll have terminal access and a filesystem. Your home directory is persisted, even when your server is terminated, and you'll have out-of-the-box access to fantastic features like a shared team volume, S3 buckets and more.</p> Benefits Details Accessible anywhere in the world Eliminates key loss and VM lockouts. Access notebook servers through a user-friendly web interface, with time-limited sharing links for convenience. Flexible resource usage Tier-based access to minimum and maximum vCPUs and memory for notebook servers. Upgrade for larger servers or use K8s for additional cluster resources. No need to reinstall. GPU access Containers enable equitable GPU sharing, making them affordable compared to the VM model. CLIMB-BIG-DATA base image is pre-configured for easy A100 utilization. Sandboxed environment for teaching and training Simplifies workshops: create a team, invite attendees, and share materials on team drive or S3. No SSH login hassles. Pre-installed software and tools CLIMB-BIG-DATA container has pre-installed Conda, Nextflow, and CLI tools, ready to use, simplifying OS setup and team integration."},{"location":"notebook-servers/read-this-first/#key-differences-to-standard-linux-installation","title":"Key differences to standard Linux installation","text":"<p>The notebook server is a virtualised installation of Linux, and as such there are some differences to a standard Linux installation.</p> Restriction Solution No system wide superuser (sudo) Install software within home directory with package managers (e.g. conda) rather than apt/yum No running of web services Static results (including HTML) can be hosted via S3. No support for opening ports This is out of scope for notebooks, but CLIMB-BIG-DATA does have short term leases for more traditional virtual machines"},{"location":"notebook-servers/read-this-first/#resource-allocation-and-usage-guidelines","title":"Resource Allocation and Usage Guidelines","text":"<p>Team members accessing the free tier quota are granted access up to a total of 14 CPUs, with 8 allocated for opening a single notebook and 6 reserved for Nextflow tasks. Additionally, teams receive 1TB of fast SSD team share storage for collaborative projects and 1TB of S3 storage for data storage needs.</p>"},{"location":"notebook-servers/read-this-first/#resource-allocation-breakdown","title":"Resource Allocation Breakdown","text":"<ul> <li>CPU Allocation:</li> <li> <p>Total Available CPUs: 14</p> <ul> <li>CPUs for Notebook Opening: 8</li> <li>CPUs Reserved for Nextflow Tasks: 6</li> </ul> </li> <li> <p>Storage Allocation:</p> </li> <li>Fast SSD Team Share Storage: 1TB</li> <li>S3 Storage: 1TB</li> </ul> <p>These resources are designed to support your team's computational and storage needs effectively. Should your team require additional resources beyond the free tier quota, please feel free to contact us for quota expansion options and pricing details.</p>"},{"location":"notebook-servers/read-this-first/#notebook-environment-and-usage","title":"Notebook Environment and Usage","text":"<ul> <li>Data and Environment Persistence: </li> <li> <p>As a user, rest assured that when you stop your notebook, your data and environment settings are safely preserved. Upon relaunching, you'll find all your files and environment configurations intact, though any ongoing processes will cease.</p> </li> <li> <p>Notebook Sharing:</p> </li> <li>When you initiate a notebook session, you have the option to share access with another team member for up to 24 hours using a shareable link. However, if your notebook is stopped, another user can launch their own instance independently.</li> <li> <p>Moreover, it's crucial to acknowledge that this feature is primarily geared towards troubleshooting and instructional use rather than practical work applications. Its primary purpose is to assist in problem-solving and enhance learning experiences, rather than being utilized as a primary tool for day-to-day tasks.</p> </li> <li> <p>Best Practices:</p> </li> <li>As a user, when considering storage options, keep in mind that Team Share Storage is primarily intended for temporary data storage. While it offers convenience and accessibility for collaborative projects, it's important to avoid extended storage durations due to its limited capacity. For long-term data storage and enhanced data safety, consider utilizing S3 buckets</li> </ul>"},{"location":"notebook-servers/read-this-first/#acclimatise-yourself-to-the-notebook-server","title":"Acclimatise yourself to the Notebook Server","text":"<p>When you first open the notebook server, you'll be presented with a JupyterLab interface. This is a web-based interface that allows you to interact with the notebook server.</p> <p></p> <p>If you are unsure how to reach this, please see the quick start guide.</p> <p>The JupyterLab interface is divided into a few areas. You'll see context menus at the top (File, Edit, View, Run etc.), a file browser pane on the left, and an activity area that initially displays a launcher interface with tiles. Clicking on one of these tiles will open a new tab in the activity area.</p> <p>These tiles are shortcuts to launch different applications. The most common one is the <code>Terminal</code>, which will open a terminal in the activity area. You can also launch a Jupyter Notebook, RStudio, or a text editor.</p> <p>You can have multiple tabs open in the activity area, and you can drag and drop them to rearrange them. You can also drag and drop files from the file browser into the activity area to open them in the appropriate application. To create a new tab, click the <code>+</code> icon in the top right of the activity area, or use <code>File &gt; New Launcher</code>.</p> <p></p> <p>For this reason, there is no option to launch multiple notebook servers from Bryn. You should be able to have all your work running in tandem in one notebook server.</p>"},{"location":"notebook-servers/read-this-first/#is-this-just-for-beginners","title":"Is this just for beginners?","text":"<p>Absolutely not. Whilst the new service is certainly far easier to get started with, it is also far more powerful for advanced users. We've ensured that each team gets a pre-mounted Kubernetes service user with permissions scoped to your team's namespace. This allows users to run Nextflow workflows on the external K8s execution environment, but also to run other containers in pods within their namespace via <code>kubectl</code>.</p>"},{"location":"notebook-servers/read-this-first/#organising-your-data-storage","title":"Organising your data storage","text":"<p>The notebook server has a few different storage locations available to you. We can explore those via the <code>terminal</code>.</p> <p></p> <p>From the terminal, you can see your home directory, which is where you'll be by default when you open the terminal. You can see the contents of your home directory by typing <code>ls</code>:</p> <pre><code>cd ~/\nls\n</code></pre> <p></p> <p>Your home directory will look similar to home directories on other UNIX-based system, as a collection of files of folders. There are two special folders, specific to notebooks:</p> <ul> <li><code>shared-team</code> - this is a symlink to the team share, which is a large, fast, shared storage location for your team. You can read and write to this folder, and it is shared with all members of your team.</li> <li><code>shared-public</code> - this is a read-only share, managed by the CLIMB-BIG-DATA team. This contains some useful data and public resources for bioinformatics workflows.</li> </ul> <p>Try listing the contents of the <code>shared-public/db</code> folder: <pre><code>ls ~/shared-public/db -l\n</code></pre> And you'll see a few folders:</p> <pre><code>total 0\ndrwxrwxr-x+  2 4744 users 620 Apr 15 12:17 blast\ndrwxrwxr-x+  2 4744 users   1 Apr 16 20:56 gtdb\ndrwxrwxr-x+ 14 4744 users  12 Apr 15 11:16 kraken2\ndrwxrwxr-x+  2 4744 users   9 Apr 16 17:36 taxonomy\n</code></pre> <p>These are large databases for bioinformatics tools. For example, the <code>kraken2</code> folder contains the databases for Kraken2 and Bracken. You can use these in your workflows, without having to download them yourself. These locations do not count to your storage quota.</p> <p>The <code>shared-team</code> folder is a good place to store your data. It is visible to everyone in your team, making it easy to collaborate on projects.</p> <p>You can list the contents of the <code>shared-team</code> folder: <pre><code>ls ~/shared-team\n</code></pre></p> <p>It will likely be empty, if this is your first time using CLIMB. You have full access to this location and can create folders and files as you wish. Try this with <code>touch</code>, that will create an empty file (if it doesn't exist). Try using <code>touch</code> on other locations, and you will see that you can only write to your home directory, the <code>shared-team</code> folder and special locations like <code>tmp/</code>.</p> <p>In locations where you can write, you will see the empty file <code>this</code> appear: <pre><code>jovyan:~$ touch ~/this\njovyan:~$ ls -l\ntotal 12\ndrwxr-sr-x 3 jovyan users 4096 Jul 19 17:02 R\nlrwxrwxrwx 1 jovyan users   14 Jul 19 18:23 shared-public -&gt; /shared/public\nlrwxrwxrwx 1 jovyan users   12 Jul 19 18:23 shared-team -&gt; /shared/team\n-rw-r--r-- 1 jovyan users    0 Jul 20 11:47 this\n-rw-r--r-- 1 jovyan users  617 Jul 20 10:53 Untitled1.ipynb\n</code></pre></p> <p>As opposed to <code>~/shared-public</code> and system locations like <code>/usr/local/bin</code> where we cannot <code>touch this</code>: <pre><code>jovyan:~$ touch ~/shared-public/this\ntouch: cannot touch '/home/jovyan/shared-public/this': Read-only file system\njovyan:~$ touch  /usr/local/bin\ntouch: setting times of '/usr/local/bin': Permission denied\n</code></pre></p> <p>For more information about data storage, please read the Understanding Storage guide.</p>"},{"location":"notebook-servers/read-this-first/#whats-next","title":"What's next?","text":"<p>The notebook server is flexible, allowing you work the way you want. You may have existing data and workflows that you want to bring to the notebook server, if so please read the Understanding Storage guide.</p> <p>You may also want to read more about the specific features of the notebook server:</p> <ul> <li>Using the Terminal</li> <li>Using Jupyter Notebooks</li> <li>Using RStudio</li> </ul> <p>Or there's the Walkthroughs, that will take you through a worked example of a bioinformatics analysis:</p> <ul> <li>Metagenomics in Brum</li> </ul>"},{"location":"notebook-servers/using-interface/","title":"Using the Jupyter Notebook Interface","text":"<p>The Jupyter notebooks are easy to use yet powerful. The interface, for example, allows to:</p> <ul> <li>Easily navigate the file system, to create, upload, download and manage files and directories.</li> <li>Have multiple tabs, that can be arranged and snapped to a corner, to a side, bottom or top of the \"snapping area\".</li> <li>Tabs can be:<ul> <li>One or more terminal tabs, to run commands and scripts.</li> <li>One or more notebook tabs, to write and run code, and to create and manage documents.</li> <li>One or more text editor tabs, to write and edit text files. You can directly edit the text (and save it) from the interface.</li> <li>Built in preview for HTML files (for example, MultiQC reports), tables (csv, tsv, etc.), and images.</li> </ul> </li> </ul> <p>The interface consists of four main components:</p> <p></p> <ol> <li>The menu bar, where you can access settings (including font size etc.), and manage the notebook.</li> <li>The side bar will allow you to select the appropriate side tab. </li> <li>The side area, which is by default set to the File Browser. From here you can navigate directories and upload/download files. You can quickly upload a file by dragging it from your computer and dropping it in the file browser.<ol> <li>Right-clicking to a file will allow you to download it, rename it, or to \"open it\" with the appropriate application.</li> </ol> </li> <li>The snapping tabs area, where you can place all your open tabs, and arrange them as you like.</li> </ol>"},{"location":"notebook-servers/using-interface/#common-tasks","title":"Common tasks","text":""},{"location":"notebook-servers/using-interface/#quickly-upload-some-files-to-the-notebook","title":"Quickly upload some files to the Notebook","text":"<ol> <li>Use the side bar (File Browser) to navigate to the appropriate location. Remember that your home directory is not a safe place for storage, and that you should use the <code>shared-team</code> directory for your data.</li> <li>If appropriate, create a new sub directory (e.g. right click in the white area of the file browser and select \"New Folder\", then type a name for it)</li> <li>On your local computer, open the directory with the files you want to upload.</li> <li>Drag the files from your local computer and drop them in the file browser of the notebook interface.</li> <li>If the file size is relevant you will get a warning. For very large uploads, you should consider using the S3 storage instead.</li> </ol>"},{"location":"notebook-servers/using-interface/#edit-a-file-and-save-it","title":"Edit a file and save it","text":"<ol> <li>Use the side bar (File Browser) to navigate to the appropriate location.</li> <li>Some files will open in an editor automatically, otherwise right click on the file and select \"Open with\" -&gt; \"Editor\".</li> <li>When you edit the file, a dot (\u26ab\ufe0f) will appear in the tab, indicating that the file has been modified.</li> <li>Use <code>Ctrl + S</code> to save the file (or <code>Cmd + S</code> on macOS), or click the save button in the editor.</li> </ol>"},{"location":"notebook-servers/using-interface/#live-preview-of-a-supported-file","title":"Live preview of a (supported) file","text":"<ol> <li>Use the side bar (File Browser) to navigate to the appropriate location.</li> <li>Right click on the file and select \"Open with\" -&gt; \"Editor\".</li> <li>Now right-click again on the file and select \"Open with\" -&gt; \"CSV Previe\" (or other supported preview: like Markdown, HTML...).</li> <li>When you edit and save the file with the editor, the preview will update automatically.</li> </ol> <p>\ud83d\udca1 You can use a similar arrangement to keep, for example, a script editor on your left and a terminal to run the script on your right. </p>"},{"location":"notebook-servers/using-interface/#using-the-python-notebook-with-custom-libraries","title":"Using the Python notebook with custom libraries","text":"<ol> <li>Open a terminal tab.</li> <li>Create a new environment with the requested libraries plus <code>ipykernel</code>, using <code>conda create -n EnvName ... ipykernel</code>. Suppose you want to create an environment called <code>dataexplorer</code> with Pandas, Seaborn, and Matplotlib, you would use <code>conda create -n dataexplorer pandas seaborn matplotlib ipykernel</code>.</li> <li>Open the launcher, and you will see a new icon with the environment name under the Python Icon. Click on it to create a new notebook with the new environment.</li> <li>From that notebook you will be able to <code>import pandas as pd</code>, for example.</li> </ol> <p>See: Using Jupyter for more details.</p>"},{"location":"notebook-servers/using-jupyter/","title":"Using Jupyter Notebooks","text":"<p>Jupyter Notebooks are a powerful and popular tool for interactive computing, data analysis, and data visualization. They allow you to create and share documents that contain live code, equations, visualizations, and explanatory text, making them an excellent choice for data scientists, researchers, and educators. In this tutorial, we'll walk you through the basics of Jupyter Notebooks and how to get started with them. There is more detailed documentation in the JupyterLab User Guide.</p> <p>Jupyter Notebooks can look very handsome, with a mix of code and formatted text. Here is an example explaining how to calculate the square root of a number.</p> <p>Unformatted (i.e. the input you would enter):</p> <p></p> <p>Formatted (i.e. the output you would see):</p> <p></p> <p>The Jupyter Notebook interface consists of four main components:</p> <ul> <li>Menu Bar: The menu bar at the top contains various options for managing the notebook, such as saving, adding new cells, changing cell types, and more.</li> <li>Toolbar: The toolbar provides quick access to frequently used actions, like running cells, saving the notebook, and adding new cells.</li> <li>File browser: The file browser, allows you to navigate directories and upload/download files.</li> <li>Notebook Area: The notebook area is where you create and edit your notebook. Each notebook is divided into a series of cells, which can be of two main types: code cells and markdown cells.</li> </ul> <p>There is a detailed debugger panel on the right side as well.</p> <p></p>"},{"location":"notebook-servers/using-jupyter/#jupyter-notebook-basics","title":"Jupyter Notebook basics","text":"<p>You primarily interact with the notebook by writing and running Code cells. Code cells in Jupyter Notebooks are the areas where you can write, execute, and interact with live code. They are one of the fundamental building blocks of Jupyter Notebooks and are used to perform computations, run algorithms, manipulate data, and create visualizations. When you run a code cell, the code inside it is executed by the kernel associated with the notebook (e.g., IPython for Python code).</p> <p>To run a code cell, select it and either press the \"Run\" button in the toolbar or use the keyboard shortcut \"Shift + Enter\". The code will be executed, and the output (if any) will be displayed below the cell. Try this now with a simple \"hello world\" example.</p> <p></p> <p>In the grey box. Type <code>print(\"Hello world\")</code> and press <code>Shift + Enter</code>. You should see the output below the cell. By pressing the plus button in the tool bar, you can add a new cell below the current one. Try this now.</p> <p></p> <p>You can also change the type of a cell from the toolbar. Try changing the type of the new cell to \"Markdown\" and typing some text (see the dropdown in the toolbar). Then run the cell to see the formatted text.</p> <p></p> <p></p> <p>Tip</p> <p>Markdown cells are used to add explanatory text, headings, images, links, and more to your notebook. You can use Markdown syntax to format the text. To render the formatted text, run the markdown cell.</p>"},{"location":"notebook-servers/using-jupyter/#saving-and-exporting-your-work","title":"Saving and exporting your work","text":"<p>To save your notebook, use either the \"Save\" option from the menu bar or press \"Ctrl + S\" or \"Cmd + S\" (on Mac) keyboard shortcuts. To export your notebook in various formats (e.g., HTML, PDF, Python script), go to \"File\" -&gt; \"Save and Export Notebook As...\" in the menu bar and select the desired format.</p>"},{"location":"notebook-servers/using-jupyter/#markdown-formatting","title":"Markdown formatting","text":"<p>These are just some of the basic Markdown syntax elements you can use to format text. Markdown allows you to easily create headings, emphasize text, create links, add images, create lists, quote text, and even include code blocks for different programming languages.</p> Markdown Syntax Output <code># Heading 1</code> # Heading 1 <code>## Heading 2</code> ## Heading 2 <code>### Heading 3</code> ### Heading 3 <code>**Bold Text**</code> Bold Text <code>*Italic Text*</code> Italic Text <code>~~Strikethrough~~</code> ~~Strikethrough~~ <code>[Link](https://climb.ac.uk/)</code> Link <code>![Image Alt Text](image.jpg)</code> <code>1. Item 1</code> 1. Item 1 <code>2. Item 2</code> 2. Item 2 <code>- Unordered List</code> - Unordered List <code>&gt; Blockquote</code> &gt; Blockquote"},{"location":"notebook-servers/using-jupyter/#additional-tips-and-tricks","title":"Additional Tips and Tricks","text":"<ul> <li>You can reorder cells by clicking and dragging them to a new position.</li> <li>To delete a cell, select it and use the \"Edit\" -&gt; \"Delete Cells\" option or press \"Ctrl + Shift + Backspace.\"</li> <li>To change the type of a cell, use the dropdown menu in the toolbar and select either \"Code\" or \"Markdown.\"</li> <li>Jupyter Notebooks support auto-completion, which can be triggered by pressing \"Tab\" while typing code.</li> <li>To get help on a function or object, append a question mark (?) at the end and run the cell.</li> </ul>"},{"location":"notebook-servers/using-jupyter/#advantages-of-jupyter-notebooks","title":"Advantages of Jupyter Notebooks","text":"<ul> <li>Interactive Computing: Jupyter Notebooks provide an interactive computing environment. Users can run code cells individually, making it easy to experiment, iterate, and visualize data in real-time.</li> <li>Data Visualization: Jupyter Notebooks support rich data visualization libraries like Matplotlib, Seaborn, and Plotly. This allows users to create interactive plots, charts, and graphs directly within the notebook.</li> <li>In-line Markdown: Notebooks support Markdown cells, allowing users to document their code and analyses with formatted text, images, links, and even equations using LaTeX syntax.</li> <li>Shareability and Reproducibility: Notebooks can be easily shared with others, facilitating collaboration and reproducibility. By sharing the code and output together, others can reproduce the same results and understand the analysis.</li> <li>Code Modularity: Notebooks allow breaking down complex analyses into smaller, more manageable code cells. This modularity makes the code easier to read, test, and maintain.</li> <li>Kernel Support: Jupyter Notebooks support multiple kernels, enabling users to work with different programming languages (Python, R, etc.) in the same environment.</li> </ul>"},{"location":"notebook-servers/using-jupyter/#installing-python-packages-in-jupyter-notebooks","title":"Installing Python packages in Jupyter Notebooks","text":"<p>Conda is the recommended way to install software on your Jupyter Notebook Servers. If you have Python packages software in a Conda environment, you  can make them available by installing <code>ipykernel</code> in that Conda environment.  To switch to the kernel associated with that environment, click on the current kernel near the top-right of the notebook (usually \"Python [conda env:root]\" or \"Python 3 (ipykernel)\") and selecting the kernel from the options under Start Preferred Kernel.  e.g., \"Python [conda env:bactopia]\".</p>"},{"location":"notebook-servers/using-nextflow/","title":"Using Nextflow","text":""},{"location":"notebook-servers/using-nextflow/#what-is-nextflow-and-how-does-it-fit-into-the-climb-big-data-platform","title":"What is Nextflow and how does it fit into the CLIMB-BIG-DATA platform?","text":"<p>In their words, \"Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.\"</p> <p>We've made Nextflow a first class citizen in CLIMB-BIG-DATA. You'll find an up-to-date stable version pre-installed in your notebook server and, more importantly, pre-configured to take advantage of our scalable Kubernetes infrastructure.</p>"},{"location":"notebook-servers/using-nextflow/#how-can-i-start-using-nextflow","title":"How can I start using Nextflow?","text":"<p>Create a notebook server and start a terminal session. Now type:</p> <pre><code>jovyan:~$ nextflow -v\n[...]\nnextflow version 23.04.0.5857\n</code></pre> <p>Your version may be newer than the above, but the point is that Nextflow is pre-installed and you should usually use this binary rather than downloading another. You don't need to follow the installation instructions in the Nextflow docs.</p>"},{"location":"notebook-servers/using-nextflow/#tutorial-using-nf-core","title":"Tutorial using nf-core","text":"<p>nf-core is a community-curated set of analysis pipelines that use Nextflow. We'll try running <code>nf-core/rnaseq</code> as an example, to demonstrate some features of how Nextflow is configured to work on CLIMB.</p> <p>We'll be using <code>-profile test</code>, which includes links to appropriate data. As a result, we'll only need to specify <code>--outdir</code> for now.</p> <pre><code>jovyan:~$ nextflow run nf-core/rnaseq -profile test --outdir nfout\n[...]\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_IDXSTATS -\n[c5/3af707] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_QUANT (RAP1_UNINDUCED_REP1)                 [ 50%] 1 of 2\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_TX2GENE                                     -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_TXIMPORT                                    -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_GENE                                     -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_GENE_LENGTH_SCALED                       -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_GENE_SCALED                              -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_TRANSCRIPT                               -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:DESEQ2_QC_STAR_SALMON                                                   -\n[9f/b3b437] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MARKDUPLICATES_PICARD:PICARD_MARKDUPLICATES (RAP1_UNINDUCED_REP2)   [  0%] 0 of 2\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MARKDUPLICATES_PICARD:SAMTOOLS_INDEX\n[...etc...]\n</code></pre>"},{"location":"notebook-servers/using-nextflow/#what-is-going-on","title":"What is going on?","text":"<p>The pipeline is executing interdependent process in parallel, using the Kubernetes executor. That is to say, rather than running inside your notebook container directly, new Kubernetes pods are being created on the fly and spinning up containers for each process.</p> <p>Open another terminal tab alongside, while the workflow is still executing, and try:</p> <pre><code>jovyan:~$ kubectl get pods\nNAME                                         READY   STATUS              RESTARTS   AGE\njupyter-demouser-2eclimb-2dbig-2ddata-2dd   1/1     Running             0          12m\nnf-0e32425fc6d3dd42c9a3cbb8dd3ccc8c          0/1     Pending             0          4s\nnf-6171723bfd88a03f1417e2aead99f180          0/1     Terminating         0          12s\nnf-0e69c114a366b717ad115431277c01d7          1/1     Running             0          9s\nnf-31f1f1f534f51863f2e19320ca7447e0          0/1     ContainerCreating   0          4s\nnf-75dc1d907794e300ff2117a49be85c63          0/1     Pending             0          4s\nnf-8491621dd73c44811c49bee448771ae5          0/1     Completed           0          13s\nnf-9aad711fc9476c27e510b9402e3089d5          0/1     ContainerCreating   0          3s\nnf-12656c698dd83e7dc2a31e3c88818227          0/1     ContainerCreating   0          4s\nnf-a57472aaef0073c9e77a0a7e6001a849          0/1     ContainerCreating   0          3s\nnf-f8d94c52e9120b7b8ba117d530f77c3a          0/1     Completed\n</code></pre> <p><code>kubectl</code> is the Kubernetes command line tool. It's also pre-installed in the CLIMB notebook environment, and pre-configured with credentials that map to a ServiceUser for your team.</p> <p>This service user has certain privileges within your namespace, or in other words an isolated part of our cluster created specifically for your team. The command <code>kubectl get pods</code> above is returning a list of pods currently running in your namespace. You'll see your notebook server (<code>jupyter-demouser-2eclimb-2dbig-2ddata-2dd</code>) and a number of Nextflow pods that are running workflow process containers. These will be in various states as they execute and then disappear.</p> <p>Once the workflow has finished, run the above command again:</p> <pre><code>jovyan:~$ kubectl get pods\nNAME                                         READY   STATUS    RESTARTS   AGE\njupyter-demouser-2eclimb-2dbig-2ddata-2dd   1/1     Running   0          17m\njovyan:~$\n</code></pre> <p>... and you're back to just your notebook server (you may also see those belonging to others in your Bryn team).</p>"},{"location":"notebook-servers/using-nextflow/#where-did-my-output-data-go","title":"Where did my output data go?","text":"<p>Once the workflow completes you'll see something like:</p> <pre><code>-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)- -[nf-core/rnaseq] Please check MultiQC report: 1/5 samples failed strandedness check.-\nCompleted at: 16-Apr-2023 13:26:51\nDuration : 5m 50s\nCPU hours : 0.4\nSucceeded : 196\n</code></pre> <p>We specified <code>nfout</code> as out outdir and you'll see the directory in the file browser on the left hand side of the JupyterLab interface. Take a look in <code>nfout/pipeline_info/</code> inside your file browser. Try double clicking on the various HTML, YAML and CSV files here and you'll see that they open in new tabs for immediate reading.</p> <p>One thing to note here, when opening HTML files such as <code>execution_report_[date].html</code>, JavaScript is disabled in the tab by default. Right click the file and select <code>Open in New Browser Tab</code> from the context window to see the full report.</p>"},{"location":"notebook-servers/using-nextflow/#where-are-nextflow-assets-and-temporaryintermediate-workdir-outputs-stored","title":"Where are Nextflow assets and temporary/intermediate (workdir) outputs stored?","text":"<p>By default, the CLIMB Nextflow config sets Nextflow 'home' to <code>/shared/team/nxf_work/$JUPYTERHUB_USER</code>. You'll see a number of subdirectories exist at that location, including <code>assets</code>, which will now contain the rnaseq workflow we just used, and <code>work</code>: where the intermediate outputs are located.</p> <pre><code>jovyan:~$ ls /shared/team/nxf_work/demouser.climb-big-data-d/\nassets  capsule  framework  plugins  secrets  tmp  work\n</code></pre>"},{"location":"notebook-servers/using-nextflow/#climb-nextflow-config-defaults","title":"CLIMB Nextflow config defaults","text":"<p>We have tried to make it as easy as possible to use Nextflow on CLIMB, and to make full use of available resources via our Kubernetes infrastructure. Out-of-the box, we set a number of configuration defaults.</p> <ul> <li>Nextflow home is set to <code>/shared/team/nxf_work/$JUPYTERHUB_USER</code></li> <li>WorkDir is set to <code>/shared/team/nxf_work/$JUPYTERHUB_USER/work</code></li> <li>Executor is set to <code>k8s</code> (plus some supporting config)</li> <li><code>/shared/team</code> and <code>/shared/public</code> (read only) are mounted as PVCs to all Nextflow pods</li> <li>A K8s ServiceUser is pre-mounted (no credentials setup required)</li> <li>S3 bucket path-style access is enabled, with <code>s3.climb.ac.uk</code> set as the endpoint</li> <li>S3 keys have also been injected from Bryn</li> </ul>"},{"location":"notebook-servers/using-nextflow/#how-to-run-nextflow-locally-with-mamba","title":"How to run NextFlow locally with Mamba.","text":"<p>When working with limited computing resources or tasks that don't require additional cores, launching new ones might not be the most efficient choice. In such cases, NextFlow offers a solution by allowing you to utilize the cores already assigned to your notebook for execution.</p> <p>To achieve this, NextFlow provides the option to specify the Mamba profile and set the process executor to local. By doing so, you can optimize resource usage and minimize unnecessary overhead.</p> <p>To run NextFlow with Mamba for nf-core pipelines, follow these steps:</p> <pre><code>nextflow run &lt;your_nfcore_pipeline.nf&gt; -profile mamba -process.executor=local\n ```\nReplace `&lt;your_nfcore_pipeline.nf&gt;` with the filename of the nf-core Pipeline you want to execute. The options `-profile` and `-process.executor` should be specified to ensure proper configuration.\nIf Mamba encounters issues with older Pipelines, you can use the -profile conda option. However, note that this may be slower:\n\n```console\nnextflow run &lt;your_nfcore_pipeline.nf&gt; -profile conda -process.executor=local\n</code></pre>"},{"location":"notebook-servers/using-rstudio/","title":"Using RStudio","text":"<p>RStudio is an integrated development environment (IDE) specifically designed for the R programming language. It provides a user-friendly interface that simplifies the process of coding, debugging, data visualization, and data analysis using R. RStudio is available in both open-source and commercial editions.</p> <p>RStudio was originally an interface to interactive with <code>R</code> programming language. Since July 2022, it is now known as Posit as they support other languages such as Python. The main feature remains the same, to easily edit scripts and view input/output files. Our version of RStudio only supports the <code>R</code> programming language. Here are the top five features of RStudio:</p> <ul> <li>Intuitive Code Editor: RStudio provides an intuitive code editor with syntax highlighting, code completion, and code folding. These features make it easier to write, edit, and navigate through R code, reducing errors and improving coding efficiency.</li> <li>Interactive Data Visualization: RStudio has an integrated plot viewer that supports interactive and high-quality data visualizations using popular R packages like ggplot2 and plotly. Users can easily explore and customize visualizations to gain insights from data.</li> <li>Workspace and Data Management: RStudio offers a workspace panel that displays all the variables and data objects currently in memory. Users can inspect, manipulate, and remove objects directly from the workspace, making it easy to manage data and perform exploratory data analysis.</li> <li>R Markdown Support: RStudio has strong support for R Markdown, a format that allows users to create dynamic documents that combine R code, visualizations, and narrative text. This feature is beneficial for creating reproducible reports, research papers, and data-driven presentations.</li> <li>Integrated R Console and Terminal: RStudio includes a built-in R console that allows users to interactively execute R code and see the output in real-time. Additionally, it provides a terminal console for executing system commands and running scripts from the command line, making it a versatile tool for data analysis and workflow management.</li> </ul> <p>If you are not familiar with <code>R</code> or RStudio you can find more information here: https://education.rstudio.com/learn/beginner/  </p>"},{"location":"notebook-servers/using-rstudio/#how-to-start","title":"How to start?","text":"<p>Both <code>R</code> and RStudio are already installed for you. You will find them under the \u201cNotebooks\u201d section of the Main Work Area. </p> <p></p> <p>For pure access to <code>R</code> you can use the Jupyter Notebook. For access to RStudio, a separate tab will be opened:</p> <p></p> <ol> <li>Editor \u2013 This is where you can interactively write and edit your scripts.</li> <li>Console \u2013 This is where your code can be run.</li> <li>Environment \u2013 This is where all input/output objects can be viewed as either Data or Values.</li> <li>Output \u2013 View plots, packages, help for package documentation.</li> </ol> <p>You can Select \u201cFile\u201d and \u201cNew File\u201d to create and open a variety of file types:</p> <p></p>"},{"location":"notebook-servers/using-rstudio/#installing-packages","title":"Installing packages","text":"<p>You can install packages which provide additional features and documentation to the base <code>R</code> packages. You can do this via the GUI or via the terminal like so:</p> <pre><code>install.packages(\"ggplot2\")\n</code></pre> <p>Once a package is installed, you will need to load it: <pre><code>library(ggplot2)\n</code></pre></p> <p>Now all of the features from <code>ggplot2</code> will be available to you.</p> <p>All plots are highly customisable, you just need to know the right functions and layering them.</p> <p>Tip</p> <p>Regarding R version control \u2013 we will only be providing the newest version of R so please do check the version for your scripts.</p> <p>Here is a simple example of a plot using the <code>ggplot2</code> package:</p> <pre><code>library(ggplot2)\nmpg\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n</code></pre> <p></p>"},{"location":"notebook-servers/using-rstudio/#other-useful-packages","title":"Other useful packages","text":"<ul> <li><code>BactDating</code> \u2013 time scaled phylogenies</li> <li><code>ggtree</code> \u2013 visualising phylogenetic trees</li> <li><code>gtsummary</code> \u2013 presentation ready data summary and analytic results tables</li> <li><code>lme4</code> \u2013 Linear Mixed-Effects Models</li> <li><code>phyloseq</code> \u2013 Handling and analysis of high-throughput microbial communities</li> <li><code>qiime2R</code> \u2013 importing Qiime output tables</li> <li><code>RcolorBrewer</code> \u2013 ColorBrewer Palettes</li> <li><code>tidyverse</code> - collection of R packages designed for data science</li> <li><code>VennDiagram</code> \u2013 Generates High-Resolution Venn and Euler Plots</li> </ul> <p>There are many \"cheat sheets\" available for individual packages which you can find here.</p>"},{"location":"notebook-servers/using-rstudio/#how-to-upload-file-to-rstudio-from-local-machine","title":"How to upload file to Rstudio from local machine","text":"<ol> <li> <p>Open the Rstudio session. </p> </li> <li> <p>Click on the Upload tab in the Output section. </p> </li> <li> <p>A pop-up window Upload Files will be open.  </p> </li> <li> <p>Click on the  Choose file and browse to the location of the file within the local machine and then select the file you want to upload and click on \"Open\". </p> </li> <li> <p>Click OK</p> </li> <li> <p>Now you can see the file in the Output section. </p> </li> </ol>"},{"location":"notebook-servers/using-the-terminal/","title":"Jump into the terminal","text":"<p>Select <code>File -&gt; New -&gt; Terminal</code>, or click the Terminal icon on the launcher pane. You'll get a new terminal tab in the activity bar, and find yourself in a bash shell.</p> <p>Who is jovyan?</p> <p>Looking at your bash prompt, you'll notice that your username is <code>jovyan</code> (there's a backstory, but it means 'related to Jupyter'). Why is everyone's username the same? Your notebook server is running as a container. The container instance is private and linked to your Bryn user's storage, but the image it runs is the same for everyone. As a result, it is not necessary or desirable to have unique system users.</p> <p>TLDR: don't worry about it. Inside your notebook server, your username is <code>jovyan</code></p> <p></p>"},{"location":"notebook-servers/using-the-terminal/#where-am-i-who-am-i","title":"Where am I? Who am I?","text":"<p>By default, you're in a bash shell running against the base operating system of the <code>climb-jupyterhub</code> container image (which is based on Ubuntu). You'll see in your bash prompt that you're in your home directory (represented by the tilde character <code>~</code>).</p> <pre><code>jovyan:~$ pwd\n/home/jovyan\n</code></pre> <p>What about sudo?</p> <p><code>jovyan</code> doesn't have sudo privileges. This may seem restrictive, but we've pre-configured the <code>climb-jupyter</code> base image with everything you'd likely need sudo for pre-installed. Everything else should be installable via package managers, such as Conda. You'll also be able to run Nextflow against out K8s execution environment 'out-of-the-box'.</p>"},{"location":"notebook-servers/using-the-terminal/#how-do-i-install-software","title":"How do I install software?","text":"<p>In the first instance, check out installing software with Conda.</p>"},{"location":"notebook-servers/using-the-terminal/#what-next","title":"What next?","text":"<p>You many want to read the reference material on using the Linux command line</p> <ul> <li>Linux Commands</li> <li>Linux Pipelines</li> <li>File Permissions in Linux</li> <li>Copying files with <code>scp</code></li> <li>Copying files with <code>rsync</code></li> </ul>"},{"location":"notebook-servers/using-vscode/","title":"Working on your notebook from Visual Studio Code","text":"<p>Visual Studio Code, or VS Code, is a very popular IDE.  In this tutorial we will see how to use VS code to connect to your CLIMB Jupyter Notebook.</p> <p>This tutorial shows you how you can configure your CLIMB BIG DATA notebook to accept a connection from your local installation of Visual Studio Code. In this way you will be able to use the code editor, the terminal, and the drag-n-drop file bar of Visual Studio Code instead of the - great but limited - web interface.</p> <p>This is a tutorial for advanced users aiming at integrating their workflow with their CLIMB BIG DATA notebook.</p> <p>Prerequisites</p> <p>We will assume you are a VS Code user already and you can install extensions and that you have a GitHub account. If you are not, you might be interested in learning why it's so popular, and maybe download it and give it ago.</p>"},{"location":"notebook-servers/using-vscode/#install-tunnels","title":"Install \"Tunnels\"","text":"<p>Further</p> <p>The enabling technology of this tutorial is described in the Developing with Remote Tunnels page of Visual Studio Code documentation.  Note that the section How can I ensure I keep my tunnel running? will not work on notebooks.</p> <p>Inside your local VS Code, install the extension <code>Remote - Tunnels</code> by Microsoft.</p> <p>After you install it, go to your Remotes tab and login using your GitHub account (you should see a Sign in using your GitHub account item in the menu).</p> <p>See the image below to locate the \"Remotes\" tab.</p> <p></p>"},{"location":"notebook-servers/using-vscode/#install-vs-code-in-your-climb-terminal","title":"Install VS Code in your CLIMB terminal","text":"<p>First, you will need to download VS Code also in your CLIMB notebook. This step has only to be done the first time.</p> <pre><code># SETUP (this has to be done once)\ncd $HOME\n# Download VS Code (this has to be done once)\nwget -O code.tgz \"https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-x64\"\n# Expand the package (this has to be done once)\ntar xvfz code.tgz\n</code></pre> <p>Now, you can run a tunnel from your CLIMB terminal, with this command:</p> <pre><code>~/code tunnel\n</code></pre> <p>This command will start an interactive configuration walk-through so keep an eye at the terminal and  answer the questions / follow the instructions.</p> <p>You will have to follow the instructions which will involve:</p> <ol> <li>Selecting GitHub as provider</li> <li>Following the provided link and pasting there the code that will appear on your CLIMB notebook</li> <li>Authorize the connection</li> <li>Return to your CLIMB notebook terminal, and give a name to the connection</li> </ol> <p>Here a typical example of the connection setup:</p> <pre><code>*\n* Visual Studio Code Server\n*\n* By using the software, you agree to\n* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license) and\n* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n*\n\u2714 How would you like to log in to Visual Studio Code? \n\u00b7 Github Account\n\nTo grant access to the server, please log into https://github.com/login/device and use code 420D-0000\n\n\u2714 What would you like to call this machine? \u00b7 jupyter-telatin-2enf\n[2024-03-18 12:25:40] info Creating tunnel with the name: jupyter-telatin-2enf\n\nOpen this link in your browser https://vscode.dev/tunnel/jupyter-telatin-2enxf\n</code></pre> <p>When you are done, you can either click on the link provided on your terminal, or refresh your tunnels list in your local VS Code, and as shown in the image above, you should see the <code>jupyter-groupname-id</code> (or custom name you gave)</p>"},{"location":"notebook-servers/using-vscode/#what-you-can-do-now","title":"What you can do now","text":"<ol> <li>Your Visual Studio Code terminal will now display your CLIMB terminal: you will find the paths and the conda environments of your notebook, and the executions will happen on your notebook.</li> <li>Your file navigation will show you your CLIMB files, and you will be able to download and upload files to your notebook dragging and dropping files from the left sidebar</li> <li>Most notably, your code editor will be Visual Studio Code, you will have the syntax highlighting, multi-edit, plug-ins and other features of Visual Studio Code to edit and visualise your CLIMB BIG DATA files.</li> </ol>"},{"location":"reference/","title":"General reference material","text":"<p>This section contains general reference material about CLIMB-BIG-DATA, and on computing and use of Linux in general.</p>"},{"location":"reference/#linux-usage","title":"Linux Usage","text":"<ul> <li>Linux Commands</li> <li>Linux Pipelines</li> <li>File Permissions in Linux</li> <li>Copying files with scp</li> </ul>"},{"location":"reference/#climb-big-data","title":"CLIMB-BIG-DATA","text":"<ul> <li>Glossary of terms</li> </ul>"},{"location":"reference/file-permissions/","title":"File permissions in Linux","text":"<p>File permissions in Linux are a crucial aspect of the operating system's security model. They determine who can access files and directories and what actions they can perform on them. Understanding and managing file permissions are essential skills for any Linux user and system administrator.</p>"},{"location":"reference/file-permissions/#file-permissions-basics","title":"File Permissions Basics","text":"<p>In Linux, every file and directory has three sets of permissions, corresponding to three different categories of users:</p> <ol> <li>Owner: The user who created the file or directory.</li> <li>Group: A group to which the file or directory belongs. Each user can be a member of multiple groups, and one of these groups is the primary group for that user.</li> <li>Others: Any user on the system who is not the owner and does not belong to the group associated with the file or directory.</li> </ol> <p>For each of these categories, there are three basic types of permissions:</p> <ol> <li>Read (r): Allows the user to view the contents of a file or list the contents of a directory.</li> <li>Write (w): Allows the user to modify the contents of a file or create, delete, and rename files within a directory.</li> <li>Execute (x): For files, allows the user to execute the file as a program. For directories, it allows the user to access its contents (i.e., <code>cd</code> into it).</li> </ol>"},{"location":"reference/file-permissions/#displaying-file-permissions","title":"Displaying File Permissions","text":"<p>You can use the <code>ls</code> command with the <code>-l</code> option to display file permissions in long format:</p> <pre><code>$ ls -l\n-rw-r--r-- 1 user group  4096 Jul 20 10:00 myfile.txt\ndrwxr-xr-x 2 user group  4096 Jul 20 10:00 mydir\n</code></pre> <p>In the output, the first column represents the file permissions. The first character indicates the file type (<code>-</code> for regular files, <code>d</code> for directories). The next nine characters are divided into three sets of three characters, representing permissions for owner, group, and others, respectively.</p>"},{"location":"reference/file-permissions/#changing-file-permissions","title":"Changing File Permissions","text":"<p>You can modify file permissions using the <code>chmod</code> command. There are two ways to specify permissions: symbolic notation and octal notation.</p>"},{"location":"reference/file-permissions/#symbolic-notation","title":"Symbolic Notation:","text":"<p>In symbolic notation, you use letters to add or remove permissions. The letters are <code>+</code> (add), <code>-</code> (remove), and <code>=</code> (set explicitly).</p> <p>For example, to give the owner read and write permissions on a file:</p> <pre><code>$ chmod u+rw myfile.txt\n</code></pre> <p>To remove execute permission for others on a directory:</p> <pre><code>$ chmod o-x mydir\n</code></pre>"},{"location":"reference/file-permissions/#octal-notation","title":"Octal Notation:","text":"<p>Octal notation represents file permissions using a three-digit number. Each digit corresponds to one of the permission sets (owner, group, others). The digits are calculated by adding the values for read (4), write (2), and execute (1) permissions.</p> <p>For example, to set read, write, and execute permissions for the owner and read-only permissions for the group and others:</p> <pre><code>$ chmod 755 myfile.txt\n</code></pre>"},{"location":"reference/file-permissions/#recursively-changing-permissions","title":"Recursively Changing Permissions","text":"<p>The <code>chmod</code> command can be used with the <code>-R</code> option to recursively change permissions for directories and their contents.</p> <pre><code>$ chmod -R 755 mydir\n</code></pre>"},{"location":"reference/file-permissions/#special-permissions-setuid-setgid-and-sticky-bit","title":"Special Permissions: Setuid, Setgid, and Sticky Bit","text":"<p>There are three special permissions that can be set on files and directories:</p> <ol> <li> <p>Setuid (SUID): When set on an executable file, it allows the user who runs the program to temporarily inherit the owner's privileges.</p> </li> <li> <p>Setgid (SGID): When set on an executable file or directory, it allows the user who runs the program or creates files within the directory to inherit the group's privileges.</p> </li> <li> <p>Sticky Bit: When set on a directory, it ensures that only the owner of a file within that directory can delete or rename it, even if others have write permissions on the directory.</p> </li> </ol> <p>Special permissions are represented by additional characters in the file permissions display.</p>"},{"location":"reference/glossary/","title":"Glossary","text":"<p>A list of terms and definitions that are regularly used to describe our cloud computing service.</p>"},{"location":"reference/glossary/#climb-big-data-terms","title":"CLIMB-BIG-DATA terms","text":"<ul> <li>Bryn \u2013 The CLIMB-BIG-DATA user portal where you can register/login and interact with our system.</li> <li>Primary users \u2013 Those with salaried positions in UK academic institutions, government agencies or healthcare systems who have the status of independent researchers and/or team leaders. These users must hold a \u201cac.uk\u201d, \u201cgov.uk\u201d or \u201cnhs.uk\u201d email account. Primary users can register for their own team.</li> <li>Secondary users \u2013 Those working under the direction of primary users who include students, post-doctoral researchers and overseas collaborators. Secondary users can only register via a primary users invitation.</li> <li>Industrial users - Users in industry should contact us to discuss terms and conditions for industrial users.</li> <li>Team Licence - Only the group admin (the primary user) can renew the three-month licence.</li> <li>Quota \u2013 The total amount of resources available to a team.</li> <li>Jupyter Notebook - A Jupyter Notebook is an interactive web-based computational environment that allows users to create and share documents containing live code, equations, visualizations, and explanatory text.</li> </ul>"},{"location":"reference/linux-commands/","title":"About the Unix Command Line","text":""},{"location":"reference/linux-commands/#introduction-to-the-unix-command-line","title":"Introduction to the Unix Command Line","text":"<p>The Unix command line is a powerful tool that allows you to interact with your computer's operating system through text-based commands. It is widely used in various platforms, including Linux and macOS. Mastering the command line can greatly enhance your productivity and enable you to perform various tasks efficiently.</p> <p>In this tutorial, we will cover the basics of the Unix command line, including navigating the file system, manipulating files and directories, and some useful commands.</p>"},{"location":"reference/linux-commands/#getting-started","title":"Getting Started","text":""},{"location":"reference/linux-commands/#opening-the-terminal","title":"Opening the Terminal","text":"<p>To access the Unix command line, you need to open the terminal or shell application on your computer. On Linux systems, you can use the built-in terminal emulator. On macOS, you can use the Terminal application, which can be found in the Utilities folder within the Applications folder.</p>"},{"location":"reference/linux-commands/#basic-commands","title":"Basic Commands","text":"<p>Let's start with some fundamental commands to get you acquainted with the Unix command line:</p> <ol> <li> <p><code>pwd</code> (Print Working Directory):    Displays the current directory (folder) you are in.    <pre><code>pwd\n/Users/username/documents\n</code></pre></p> </li> <li> <p><code>ls</code> (List):    Lists the files and directories in the current directory.    <pre><code>ls\nfile1.txt  file2.txt  folder1  folder2\n</code></pre></p> </li> <li> <p><code>cd</code> (Change Directory):    Allows you to navigate to a different directory.    <pre><code>cd folder1\n</code></pre></p> </li> <li> <p><code>mkdir</code> (Make Directory):    Creates a new directory.    <pre><code>mkdir new_folder\n</code></pre></p> </li> <li> <p><code>touch</code>:    Creates an empty file.    <pre><code>touch new_file.txt\n</code></pre></p> </li> <li> <p><code>rm</code> (Remove):    Deletes files or directories.    <pre><code>rm file1.txt\nrm -r folder1\n</code></pre></p> </li> <li> <p><code>mv</code> (Move):    Moves or renames files and directories.    <pre><code>mv file2.txt folder2/\nmv file2.txt new_filename.txt\n</code></pre></p> </li> <li> <p><code>cp</code> (Copy):    Copies files or directories.    <pre><code>cp file1.txt folder2/\n</code></pre></p> </li> </ol>"},{"location":"reference/linux-commands/#navigating-the-file-system","title":"Navigating the File System","text":"<p>The Unix file system is organized in a hierarchical structure. Each directory can contain files and other directories. Here are some additional commands to help you navigate the file system:</p> <ol> <li> <p><code>cd</code> (Change Directory):    As mentioned earlier, <code>cd</code> allows you to change your current directory. You can use absolute or relative paths.    <pre><code>cd /path/to/directory  # Absolute path\ncd ../parent_directory  # Relative path (move up one level)\ncd folder3/subfolder   # Relative path (move down into subfolder)\n</code></pre></p> </li> <li> <p><code>ls</code> (List):    You can use various options with <code>ls</code> to customize the output:    <pre><code>ls -l  # Long listing format with detailed information\nls -a  # List all files, including hidden ones (those starting with .)\nls -h  # Human-readable file sizes\nls *.txt  # List all files ending with .txt\n</code></pre></p> </li> <li> <p><code>pwd</code> (Print Working Directory):    Useful to check where you are in the file system.</p> </li> <li> <p><code>..</code> and <code>.</code>:    <code>..</code> refers to the parent directory, while <code>.</code> refers to the current directory. They can be handy when constructing paths.</p> </li> </ol>"},{"location":"reference/linux-commands/#file-manipulation","title":"File Manipulation","text":"<p>Now let's explore some additional commands to manipulate files:</p> <ol> <li> <p><code>cat</code> (Concatenate):    Displays the content of a file.    <pre><code>cat file1.txt\n</code></pre></p> </li> <li> <p><code>less</code> and <code>more</code>:    Allow you to view the content of large files interactively.    <pre><code>less large_file.txt\nmore another_large_file.txt\n</code></pre>    Use the arrow keys to scroll, and press <code>q</code> to exit.</p> </li> <li> <p><code>head</code> and <code>tail</code>:    Display the beginning or end of a file.    <pre><code>head file1.txt  # Show the first few lines\ntail file1.txt  # Show the last few lines\n</code></pre></p> </li> <li> <p><code>grep</code>:    Searches for a pattern in a file.    <pre><code>grep \"keyword\" file1.txt\n</code></pre></p> </li> <li> <p><code>wc</code> (Word Count):    Counts the number of lines, words, and characters in a file.    <pre><code>wc file1.txt\n</code></pre></p> </li> </ol>"},{"location":"reference/linux-pipelines/","title":"About Linux Pipelines","text":"<p>Pipelining, in the context of the Unix command line, refers to the practice of connecting multiple commands together in a chain, where the output of one command becomes the input of the next command. This technique allows users to perform complex operations and data processing efficiently by breaking them down into smaller, focused tasks. The concept of pipelining is one of the most powerful features of the command line and greatly enhances productivity.</p>"},{"location":"reference/linux-pipelines/#how-pipelining-works","title":"How Pipelining Works","text":"<p>The key to pipelining is the use of the vertical bar symbol (<code>|</code>). When you use the pipe symbol in a command, it takes the output of the preceding command and sends it as the input to the following command. The data is passed through the pipe from left to right, allowing each command in the pipeline to process it sequentially.</p> <p>The general syntax for pipelining is as follows:</p> <pre><code>command1 | command2 | command3 ...\n</code></pre> <p>The output of <code>command1</code> becomes the input of <code>command2</code>, and the output of <code>command2</code> becomes the input of <code>command3</code>, and so on.</p>"},{"location":"reference/linux-pipelines/#benefits-of-pipelining","title":"Benefits of Pipelining","text":"<p>Pipelining offers several advantages:</p> <ol> <li> <p>Modularity: You can break down complex tasks into smaller, manageable steps by using multiple commands in the pipeline. Each command performs a specific operation, making the overall process more organized and easier to maintain.</p> </li> <li> <p>Efficiency: Rather than generating and storing intermediate files, pipelining processes data in real-time. This approach reduces disk I/O and saves storage space, resulting in faster and more resource-efficient data processing.</p> </li> <li> <p>Flexibility: You can combine existing commands or custom scripts in a pipeline to achieve specific outcomes. This flexibility allows you to tailor the process to your needs, regardless of the complexity of the task.</p> </li> <li> <p>Reusability: Pipelines are easily reusable. Once you have constructed a pipeline that performs a particular task, you can reuse it with different input data without modifying the individual commands.</p> </li> <li> <p>Automation: Pipelining is conducive to automation, as it enables you to create scripts and automate repetitive tasks, saving time and effort.</p> </li> </ol>"},{"location":"reference/linux-pipelines/#examples-of-pipelining","title":"Examples of Pipelining","text":"<p>Let's look at some practical examples of pipelining:</p>"},{"location":"reference/linux-pipelines/#example-1-sorting-and-displaying","title":"Example 1: Sorting and Displaying","text":"<p>Suppose you have a text file named <code>data.txt</code>, and you want to sort its lines in alphabetical order and then display the first ten lines:</p> <pre><code>$ sort data.txt | head -n 10\n</code></pre> <p>Here, the <code>sort</code> command sorts the content of <code>data.txt</code>, and the sorted output is passed through the pipe to the <code>head</code> command, which displays the first ten lines.</p>"},{"location":"reference/linux-pipelines/#example-2-filter-and-count","title":"Example 2: Filter and Count","text":"<p>Suppose you have a log file named <code>server.log</code>, and you want to count how many times a particular error message occurs:</p> <pre><code>$ grep \"error\" server.log | wc -l\n</code></pre> <p>Here, the <code>grep</code> command searches for lines containing the word \"error\" in <code>server.log</code>, and the resulting lines are sent to <code>wc</code> (word count) with the <code>-l</code> option to count the number of occurrences.</p>"},{"location":"reference/linux-pipelines/#example-3-download-and-extract","title":"Example 3: Download and Extract","text":"<p>Suppose you want to download a compressed file from a URL and extract its contents directly without saving the file locally:</p> <pre><code>$ curl -s https://example.com/file.tar.gz | tar xz\n</code></pre> <p>Here, <code>curl</code> downloads the file and passes its output (compressed data) through the pipe to <code>tar</code>, which extracts the contents (<code>x</code> for extract, <code>z</code> for gzip, <code>s</code> for silent mode).</p>"},{"location":"reference/rsync/","title":"About Rsync","text":"<p>What is Rsync?</p> <p>Rsync (Remote Sync) is a powerful command-line utility used for file synchronization and transfer between local and remote systems. It efficiently copies and synchronizes files, directories, or even whole file systems with minimal data transfer. Rsync is widely used for backup, mirroring, and data migration tasks due to its speed, reliability, and ability to resume interrupted transfers.</p> <p>Basic Syntax:</p> <p>The basic syntax of rsync is as follows:</p> <pre><code>rsync [options] source destination\n</code></pre> <ul> <li><code>source</code>: The path of the files or directories you want to synchronize or copy.</li> <li><code>destination</code>: The target location where you want to copy the data.</li> </ul> <p>Common Options:</p> <ul> <li> <p><code>-a</code> (archive): This option preserves permissions, timestamps, symbolic links, and recursive copying. It is often used for typical backup and synchronization tasks.</p> </li> <li> <p><code>-v</code> (verbose): Enables verbose output, providing more information about the files being transferred.</p> </li> <li> <p><code>-z</code> (compress): Compresses data during transfer, which can be useful for slow or limited bandwidth connections.</p> </li> <li> <p><code>--delete</code>: Deletes files in the destination that are not present in the source. Use this with caution as data loss can occur if not used carefully.</p> </li> <li> <p><code>-P</code> (progress): Displays progress during the transfer, showing the percentage completed and estimated time.</p> </li> </ul> <p>Examples:</p> <ol> <li>Copy local files/directories to a remote server:</li> </ol> <pre><code>rsync -avz /path/to/local/source user@remote_server:/path/to/destination\n</code></pre> <ol> <li>Copy files/directories from a remote server to the local system:</li> </ol> <pre><code>rsync -avz user@remote_server:/path/to/source /path/to/local/destination\n</code></pre> <ol> <li>Synchronize two directories (local or remote):</li> </ol> <pre><code>rsync -avz /path/to/source/ /path/to/destination/\n</code></pre> <p>Exclude Files or Directories:</p> <p>You can exclude certain files or directories from the rsync process using the <code>--exclude</code> option. This is particularly useful when you want to omit specific files or directories from being synchronized.</p> <pre><code>rsync -avz --exclude 'file_to_exclude' /path/to/source/ /path/to/destination/\n</code></pre> <p>Dry Run:</p> <p>Before performing the actual sync, you can use the <code>--dry-run</code> option to see what rsync would do without actually making any changes. This allows you to preview the outcome and verify if the command behaves as expected.</p> <pre><code>rsync -avz --dry-run /path/to/source/ /path/to/destination/\n</code></pre>"},{"location":"reference/scp/","title":"Tutorial: Secure Copy (SCP) - Transfer Files Securely Over SSH","text":"<p>SCP (Secure Copy) is a command-line utility that allows you to securely transfer files and directories between a local host and a remote host or between two remote hosts over a Secure Shell (SSH) connection. It provides a secure and encrypted way to transfer data, making it a valuable tool for remote file transfers and backups.</p> <p>In this tutorial, you'll learn how to use SCP to transfer files between various scenarios:</p> <ol> <li>Copy files from your local machine to a remote server.</li> <li>Copy files from a remote server to your local machine.</li> <li>Copy files between two remote servers.</li> </ol>"},{"location":"reference/scp/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the tutorial, make sure you have the following:</p> <ol> <li>Access to a local machine with SCP installed (SCP comes pre-installed on most Unix-based systems).</li> <li>Access to a remote server with SSH enabled.</li> </ol>"},{"location":"reference/scp/#syntax","title":"Syntax","text":"<p>The basic syntax of SCP is as follows:</p> <pre><code>scp [options] source destination\n</code></pre> <ul> <li><code>source</code>: The file or directory you want to copy. It can be a local file/directory or a remote file/directory in the format <code>username@remote_host:file_or_directory</code>.</li> <li><code>destination</code>: The target location where the file/directory should be copied. It can be a local path or a remote path.</li> </ul>"},{"location":"reference/scp/#examples","title":"Examples","text":""},{"location":"reference/scp/#1-copy-local-file-to-a-remote-server","title":"1. Copy local file to a remote server:","text":"<pre><code>scp /path/to/local/file.txt username@remote_host:/path/on/remote/server/\n</code></pre> <p>Replace <code>/path/to/local/file.txt</code> with the local file you want to transfer and <code>username@remote_host</code> with your remote server's SSH credentials. Also, specify the destination directory on the remote server where you want to copy the file.</p>"},{"location":"reference/scp/#2-copy-a-remote-file-to-your-local-machine","title":"2. Copy a remote file to your local machine:","text":"<pre><code>scp username@remote_host:/path/on/remote/server/file.txt /path/to/local/\n</code></pre> <p>Replace <code>username@remote_host</code> with your remote server's SSH credentials, and specify the path of the remote file you want to copy. Also, specify the destination directory on your local machine where you want to save the file.</p>"},{"location":"reference/scp/#3-copy-a-directory-from-local-to-remote-server","title":"3. Copy a directory from local to remote server:","text":"<pre><code>scp -r /path/to/local/directory username@remote_host:/path/on/remote/server/\n</code></pre> <p>Use the <code>-r</code> option to recursively copy a directory and its contents.</p>"},{"location":"reference/scp/#4-copy-a-directory-from-remote-to-local-machine","title":"4. Copy a directory from remote to local machine:","text":"<pre><code>scp -r username@remote_host:/path/on/remote/server/directory /path/to/local/\n</code></pre> <p>Use the <code>-r</code> option for recursive copying when transferring directories.</p>"},{"location":"reference/scp/#5-copy-files-between-two-remote-servers","title":"5. Copy files between two remote servers:","text":"<pre><code>scp username@remote_host1:/path/to/source/file.txt username@remote_host2:/path/to/destination/\n</code></pre> <p>Replace <code>username@remote_host1</code> and <code>username@remote_host2</code> with the respective remote servers' SSH credentials. Specify the source file on the first remote server and the destination directory on the second remote server.</p>"},{"location":"reference/scp/#additional-options","title":"Additional Options","text":"<p>SCP provides some useful options for different scenarios. Here are a few commonly used ones:</p> <ul> <li><code>-P</code>: Specify a custom SSH port.</li> <li><code>-i</code>: Use a specific identity file (private key) for SSH authentication.</li> <li><code>-C</code>: Compresses data during the transfer, reducing the transfer time.</li> <li><code>-q</code>: Suppress SCP's progress meter and non-error messages.</li> <li><code>-p</code>: Preserve file attributes, such as timestamps and permissions.</li> </ul>"},{"location":"storage/","title":"How to transfer data between CLIMB-BIG-DATA and other systems","text":"<p>This section of the documentation will explain how to transfer data between CLIMB-BIG-DATA and other systems. It will cover the following topics:</p> <ul> <li>Transferring from your local machine to and from your CLIMB-BIG-DATA notebook server</li> <li>Transferring within your CLIMB-BIG-DATA notebook server (S3 to notebook)</li> <li>Transferring from your local machine to the CLIMB-BIG-DATA S3 buckets</li> <li>Transferring programmatically between an S3 bucket and CLIMB-BIG-DATA notebook server (using python)</li> </ul> <p>Tip</p> <p>For best performance, we recommend storing primary data (sequenced reads) on S3 buckets and pulling them down on demand for analysis. This can  be automated with workflow languages such as Nextflow. </p> <p>If you are moving from an older CLIMB VM to the new notebook model, you can read a dedicated guide here.</p> <p>To first understand how to transfer data, it is important to understand how data is stored in CLIMB-BIG-DATA. This is covered in the next section.</p>"},{"location":"storage/#understanding-storage","title":"Understanding storage","text":"<p>Data in CLIMB-BIG-DATA can be found in four major locations:</p> <ul> <li>Your home directory <code>/home/jovyan</code> (~)</li> <li>A writeable 'team share' mounted at <code>~/shared-team/</code> and linked to your home directory as <code>shared-team</code></li> <li>Read-only shares mounted at <code>/shared/public</code> and linked to your home directory as <code>shared-public</code></li> <li>S3 buckets</li> </ul> <p>These interact with each other as shown in the diagram below:</p> <p></p>"},{"location":"storage/#home-directory","title":"Home directory","text":"<p>The home directory <code>/home/jovyan</code> (<code>~</code>) is mounted to persistent storage for the CLIMB user, ensuring data retention after container restarts. It is a small working directory location. Its intentional small size (usually 20GB) makes it unsuitable for large Conda environments or databases. Alternative storage options are available. Additionally, the home directory serves as the default/base location for the file browser pane.</p>"},{"location":"storage/#team-share-shared-team","title":"Team share (shared-team)","text":"<p>A writeable 'team share' is accessible at <code>/shared/team</code>, symlinked to your home directory as <code>shared-team</code> for easy file browsing. It offers significant storage space (1TB+ depending on tier) and grants all team members simultaneous read/write access. Additionally, it benefits from SSD-backed technology, ensuring exceptionally fast performance.</p>"},{"location":"storage/#s3-buckets","title":"S3 Buckets","text":"<p>When using S3 buckets via Bryn, your access keys will be automatically injected into the notebook server as environment variables. The <code>aws cli</code> and <code>s3cmd</code> have been pre-configured to utilize the CLIMB S3 endpoints by default. This means you can access S3 buckets without any additional setup or configuration required.</p>"},{"location":"storage/#public-shared-shared-public","title":"Public shared (shared-public)","text":"<p>In addition to the team share, you may also notice additional mounts under <code>/shared/</code>, including at least <code>/shared/public</code>. Here you will find read-only data and resources provided by CLIMB, that may be useful to microbial bioinformatics workflows. Initially we have populated these shares with a few key resources:</p> <ul> <li>Ben Langmead's Kraken2/Bracken Refseq Databases - in <code>/shared/public/db/kraken2</code></li> <li>The NCBI GenBank non-redundant protein BLAST database - in <code>/shared/public/db/blast</code></li> </ul>"},{"location":"storage/#what-is-an-s3-bucket-object-storage","title":"What is an S3 bucket (Object storage)?","text":"<p>Object storage, implemented here as S3 buckets, is a way to store lots of data in a simple and flexible manner. Instead of organizing data into folders and files like on a computer, it treats each piece of data as a separate \"object\" with a unique name. These objects can be anything, like pictures, videos, or documents. Object storage is highly scalable, meaning it can handle a huge amount of data without slowing down. It also keeps copies of data to make sure it doesn't get lost. This type of storage is often used in cloud computing because it's cost-effective and easy to manage.</p>"},{"location":"storage/fetch-s3-to-notebook/","title":"Transferring within your CLIMB-BIG-DATA notebook server (S3 to notebook)","text":"<p>The easiest and ready way to transfer to/from your notebook to the S3 buckets is to use <code>s3cmd</code> which is already installed on your notebook server. <code>s3cmd</code> is a command-line tool used to interact with Amazon Simple Storage Service (S3). It allows you to manage your S3 buckets and objects, including uploading, downloading, and deleting files. </p> <p>If you've started using S3 buckets via Bryn, your access keys will have already been injected into the notebook server as environment variables. We've also pre-configured <code>aws cli</code> and <code>s3cmd</code> to use the CLIMB S3 endpoints by default. As a result, you should be able to access buckets with no additional setup.</p> <p>Your S3 bucket storage is pre-configured for you through the <code>$AWS_SECRET_ACCESS_KEY</code> and <code>$AWS_ACCESS_KEY_ID</code> variables and the configuration file <code>~/.s3cfg</code>. Do not change any of these unless you are very confident with S3 and can use it without support. The command line tool <code>s3cmd</code> provides convenient access for moving files around. Here are some commands to get you started.</p> <p>If you are moving from an older CLIMB VM to the new notebook model, you can read a dedicated guide here.</p>"},{"location":"storage/fetch-s3-to-notebook/#list-s3-buckets","title":"List S3 buckets","text":"<p>To list all your S3 buckets, use the following command:</p> <pre><code>s3cmd ls\n</code></pre>"},{"location":"storage/fetch-s3-to-notebook/#upload-a-file","title":"Upload a file","text":"<p>To upload a file to an S3 bucket, use the following command:</p> <pre><code>s3cmd put /path/to/local/file s3://bucket-name/destination/path/\n</code></pre>"},{"location":"storage/fetch-s3-to-notebook/#download-a-file","title":"Download a file","text":"<p>To download a file from an S3 bucket, use the following command:</p> <pre><code>s3cmd get s3://bucket-name/path/to/s3/file /path/to/local/destination/\n</code></pre>"},{"location":"storage/fetch-s3-to-notebook/#copy-a-file-within-s3","title":"Copy a file within S3","text":"<p>To copy a file from one location to another within the same S3 bucket, use the following command:</p> <pre><code>s3cmd cp s3://bucket-name/source/path/to/s3/file s3://bucket-name/destination/path/\n</code></pre>"},{"location":"storage/fetch-s3-to-notebook/#delete-a-file","title":"Delete a file","text":"<p>To delete a file from an S3 bucket, use the following command:</p> <pre><code>s3cmd del s3://bucket-name/path/to/s3/file\n</code></pre>"},{"location":"storage/fetch-s3-to-notebook/#synchronize-local-directory-with-s3-bucket","title":"Synchronize local directory with S3 bucket","text":"<p>To synchronize a local directory with an S3 bucket (upload only the changed files), use the following command:</p> <pre><code>mkdir ~/mys3bucket\necho \"this is local\" &gt; ~/mys3bucket/testLocal.txt\ns3cmd sync ~/mys3bucket/ s3://teamname-bucketname1/path_to_sync_with/\n</code></pre>"},{"location":"storage/fetch-s3-to-notebook/#additional-options","title":"Additional Options","text":"<p><code>s3cmd</code> provides many more options and features. You can explore them by running:</p> <pre><code>s3cmd --help\n</code></pre>"},{"location":"storage/program-s3-fetch/","title":"Transferring programmatically between an S3 bucket and CLIMB-BIG-DATA notebook server (using python)","text":""},{"location":"storage/program-s3-fetch/#creating-an-environment","title":"Creating an environment","text":"<p>In the terminal, create a new environment called <code>s3training</code> and activate it:</p> <pre><code>conda create --solver libmamba -y -n s3training ipykernel\nconda activate s3training\n</code></pre> <p>Once this is complete, you should see the environment available as a notebook in the Launcher window (<code>File &gt; New Launcher</code>)</p> <p></p> <p>Click the tile and open a new Jupyter notebook. You should see s3training in as the name of the kernel in the top right of the notebook.</p> <p></p>"},{"location":"storage/program-s3-fetch/#uploading-and-downloading-a-file-to-s3","title":"Uploading and downloading a file to S3","text":"<p>Let's create a test file to up load to a bucket. In the Jupyter notebook, create a new cell and add the following:</p> <pre><code>!source ~/.bashrc\n!echo \"Bless this mess\" &gt; s3test.txt\n</code></pre> <p>This creates a text file called <code>s3test.txt</code> with the text \"Bless this mess\".</p> <p>Tip</p> <p>The ! allows us to run a shell command in the notebook. You could also do this in the terminal directly.</p> <p>We will need to <code>boto3</code> and <code>dotenv</code> Python module which we can install in a new cell with</p> <pre><code>pip install boto3 python-dotenv\n</code></pre> <p>Create another cell and add the following script. This will upload the file <code>s3test.txt</code> to the bucket. You will need have a bucket already created in Bryn. If you don't have one, you can create one in the Bryn dashboard. The script below has MY OWN bucket 'quadram-bioinfo-training' as the BUCKET_NAME, you will need to change this to the name of YOUR bucket.</p> <pre><code>import boto3\nimport os \nfrom dotenv import load_dotenv\n\ndotenv_path = '/home/jovyan/.s3cfg' # ~/.s3cfg is the default location for the s3 config file\nload_dotenv(dotenv_path=dotenv_path)\n\nBUCKET_NAME = 'quadram-bioinfo-training'\n\ndef upload_file_to_s3(file_name, bucket):\n    object = s3.Object(bucket, file_name)\n    object.put(Body=open(file_name, 'rb'))\n\ndef download_file_from_s3(key, bucket, output_path):\n    object = s3.Object(bucket, key)\n    with open(output_path, 'w') as f:\n         f.write(object.get()['Body'].read().decode('utf-8'))\n\n# Create a resource using your S3 credentials\ns3 = boto3.resource('s3',\n  endpoint_url = 'https://s3.climb.ac.uk',\n  aws_access_key_id = os.environ.get(\"access_key\"),\n  aws_secret_access_key = os.environ.get(\"secret_key\")\n)\n\n# Upload our file to the bucket \nupload_file_to_s3('s3test.txt', BUCKET_NAME)\n\n# Let's print the output of the bucket\nprint('Objects:')\nmy_bucket = s3.Bucket(BUCKET_NAME)    \nfor my_bucket_object in my_bucket.objects.all():\n    print(my_bucket_object.key)\n\n# Let's download our file from the bucket to another file\ndownload_file_from_s3('s3test.txt', BUCKET_NAME, 's3_backagain.txt')\n</code></pre> <p>This Python code uses the <code>boto3</code> library to interact with CLIMB's S3, which is a cloud storage service. The code defines two functions: <code>upload_file_to_s3()</code> and <code>download_file_from_s3()</code>. The <code>upload_file_to_s3()</code> function uploads a file to the specified S3 bucket, while the <code>download_file_from_s3()</code> function downloads a file from the specified S3 bucket to a local file.</p> <p>The code creates a resource object that connects to the S3 service using the specified endpoint URL and access keys. The <code>os.environ.get()</code> function is used to retrieve the values of the access key and secret key from the environment variables. These keys are used to authenticate the connection to the S3 service.</p> <p>The code then uploads a file called <code>s3test.txt</code> to the specified S3 bucket using the <code>upload_file_to_s3()</code> function. It then prints the keys of all objects in the bucket using the <code>my_bucket.objects.all()</code> function, to show the file has been uploaded;. </p> <p>Finally, the code downloads the <code>s3test.txt</code> file from the S3 bucket to a local file called <code>s3_backagain.txt</code> using the <code>download_file_from_s3()</code> function.</p> <p>As the <code>s3_backagain.txt</code> file, is just a copy of the original <code>s3test.txt</code> with a different name, it should contain the text \"Bless this mess\". i.e.</p> <pre><code>!cat 's3_backagain.txt'\n</code></pre> <p>Should show</p> <pre><code>Bless this mess\n</code></pre>"},{"location":"storage/program-s3-fetch/#reading-directly-from-s3","title":"Reading directly from S3","text":"<p>You can read directly with the <code>s3_client</code> object, without having to download the file to your notebook.  For example, to read the file we just uploaded:</p> <pre><code>object = s3.Object(BUCKET_NAME, 's3test.txt')\nprint(object.get()['Body'].read().decode('utf-8'))\n</code></pre> <p>Should return  <pre><code>Bless this mess\n</code></pre></p>"},{"location":"storage/transfer-from-vm-to-s3/","title":"Transferring data from CLIMB VMs to S3 Buckets","text":"<p>This is a dedicated guide if you are moving from using CLIMB VMs to the new notebook model. Here we will show you how to get <code>s3cmd</code> setup on your VM, create a bucket, and how to transfer data from your VM to S3.</p> <p>Tip</p> <p>This will work on any Linux machine, not just CLIMB VMs.</p>"},{"location":"storage/transfer-from-vm-to-s3/#what-is-s3","title":"What is S3?","text":"<p>S3 is a cloud-based object storage service. An S3 bucket is a fundamental container for storing and organizing data in S3. It is conceptually similar to a directory or folder in a file system, but it is designed to store objects (files) in a highly scalable and durable manner.</p>"},{"location":"storage/transfer-from-vm-to-s3/#installing-s3cmd","title":"Installing s3cmd","text":"<p>You will need to install <code>s3cmd</code> on your CLIMB VM. You can do this using python pip. I have installed it in a Conda environment but you can install <code>s3cmd</code> anywhere. For example,</p> <pre><code>conda create -n s3cmd -y\nconda activate s3cmd\npip install -U s3cmd\n</code></pre>"},{"location":"storage/transfer-from-vm-to-s3/#configuring-s3cmd","title":"Configuring s3cmd","text":"<p>You will need to configure <code>s3cmd</code> to use your CLIMB S3 credentials. You can do this by running the following command:</p> <pre><code>s3cmd --configure\n</code></pre> <p>This will then take you through a series of prompts. You can accept the defaults for all of these except for the following:</p> <pre><code>Access Key: Your access key, as displayed in BRYN\nSecret Key: Your access key, as displayed in BRYN\nS3 Endpoint [s3.amazonaws.com]: s3.climb.ac.uk\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.s3.climb.ac.uk\n</code></pre> <p>Your access and secret keys are unique to you and can be found in BRYN. These are available in the S3 buckets section of BRYN. Click on the API keys button to reveal your keys.</p> <p></p> <p>Step through all the other prompts. If the configuration is successful, you should be able to list your buckets, with <code>s3cmd ls</code></p> <pre><code>s3cmd ls\n</code></pre>"},{"location":"storage/transfer-from-vm-to-s3/#creating-a-bucket","title":"Creating a bucket","text":"<p>If you have not created a bucket for your data, you should do so now in the BRYN interface. You can do this by clicking on the New bucket button.</p> <p></p> <p>The create bucket interface will then appear. You should give your bucket a name. You can also choose to make your bucket public.</p> <p></p>"},{"location":"storage/transfer-from-vm-to-s3/#a-worked-example","title":"A worked example","text":"<p>Now with <code>s3cmd</code> ready to go, I will demonstrate how to transfer data with a worked example. This worked example uses my own (important) data, that I am transferring from my VM to S3. I have a output of a recent run of ClonalFrameML on my VM, and I want to transfer it to S3.</p> <p>We will need to correct name for our bucket which we can see using <code>s3cmd ls</code></p> <pre><code>s3cmd ls\n</code></pre> <p>Will give a list of buckets like;</p> <pre><code>2023-07-21 08:50  s3://quadram-bioinfo-training\n</code></pre> <p>Here is the directory listing of my data I want to transfer:</p> <pre><code>(s3cmd) ubuntu@chomp:~/scratch/nabil_back$ ls clonal_heidl/ -Rl\nclonal_heidl/:\ntotal 3513600\n-rwxr-xr-x 1 ubuntu ubuntu 3546608056 Apr 14  2020 clean.full.aln\n-rwxr-xr-x 1 ubuntu ubuntu   37881459 Apr 14  2020 clonal.ML_sequence.fasta\n-rwxr-xr-x 1 ubuntu ubuntu      66719 Apr 14  2020 clonal.em.txt\n-rwxr-xr-x 1 ubuntu ubuntu      15947 Apr 14  2020 clonal.importation_status.txt\n-rwxr-xr-x 1 ubuntu ubuntu      32169 Apr 14  2020 clonal.labelled_tree.newick\n-rwxr-xr-x 1 ubuntu ubuntu    9671724 Apr 14  2020 clonal.position_cross_reference.txt\n-rwxr-xr-x 1 ubuntu ubuntu      28846 Apr 14  2020 iqtree_fast.treefile\n</code></pre> <p>To transfer this to S3, we can use the <code>s3cmd sync</code> command. This will transfer the data to S3, and will only transfer files that have changed. The syntax for s3cmd sync is <code>s3cmd sync &lt;source&gt; &lt;destination&gt;</code>. In this case, we want to transfer the data from our VM to S3, so we will use the following command:</p> <pre><code>s3cmd sync clonal_heidl/  s3://quadram-bioinfo-training/clonal_heidl/\n</code></pre> <p>Note</p> <p>The destination is the bucket address we saw earlier, with <code>s3cmd ls</code>.</p> <p>Warning</p> <p>The last slash for folder name after the bucket name is important. It is how <code>s3cmd</code> knows you want to transfer into a folder. If you miss this off, you will get an error message like this: <code>Parameter problem: Destination S3 URI must end with '/' (ie must refer to a directory on the remote side).</code></p> <p>The transfer will now begin. You will see a progress bar for each file that is transferred.</p> <pre><code>(s3cmd) ubuntu@chomp:~/scratch/nabil_back$ s3cmd sync clonal_heidl/  s3://quadram-bioinfo-training/clonal_heidl/\nupload: 'clonal_heidl/clean.full.aln' -&gt; 's3://quadram-bioinfo-training/clean.full.aln'  [part 1 of 226, 15MB] [1 of 8]\n 15728640 of 15728640   100% in    1s     9.95 MB/s  done\n    .................\nDone. Uploaded 3594366897 bytes in 330.1 seconds, 10.38 MB/s.\n</code></pre> <p>Once the transfer is complete, you can check that the data is in S3 by listing the contents of the bucket. For me, I do the following:</p> <pre><code>s3cmd ls s3://quadram-bioinfo-training/clonal_heidl/\n</code></pre> <p>Shows (for me): <pre><code>2023-07-24 13:51   3546608056  s3://quadram-bioinfo-training/clonal_heidl/clean.full.aln\n2023-07-24 13:51     37881459  s3://quadram-bioinfo-training/clonal_heidl/clonal.ML_sequence.fasta\n2023-07-24 13:51        66719  s3://quadram-bioinfo-training/clonal_heidl/clonal.em.txt\n2023-07-24 13:51        15947  s3://quadram-bioinfo-training/clonal_heidl/clonal.importation_status.txt\n2023-07-24 13:51        32169  s3://quadram-bioinfo-training/clonal_heidl/clonal.labelled_tree.newick\n2023-07-24 13:51      9671724  s3://quadram-bioinfo-training/clonal_heidl/clonal.position_cross_reference.txt\n2023-07-24 13:51        28846  s3://quadram-bioinfo-training/clonal_heidl/iqtree_fast.treefile\n</code></pre></p> <p>To check one of my text files are ok, I can just read the file back to the screen (i.e. STDOUT) using <code>s3cmd get</code></p> <pre><code>s3cmd get s3://quadram-bioinfo-training/clonal_heidl/clonal.em.txt --no-progress  - | more\n</code></pre> <p>Which will give me:</p> <pre><code>Parameter       Posterior Mean  Posterior Variance      a_post  b_post\nR/theta 0.0829423       4.28704e-06     1604.7  19347.2\n1/delta 0.000825935     4.25106e-10     1604.7  1.94289e+06\nnu      0.0102149       5.26024e-09     19836.3 1.9419e+06\nERR034167       8.68129e-06     1.8105e-12      41.6265 4.79497e+06\nERR212540       1.5101e-05      3.14957e-12     72.4034 4.79462e+06\n.....\n</code></pre> <p>Success! My files are now on the S3 bucket.</p>"},{"location":"storage/transfer-from-vm-to-s3/#fetching-files-from-the-s3-bucket","title":"Fetching files from the S3 bucket","text":"<p>You can easily fetch files from the S3 bucket using <code>s3cmd get</code>. For example, to get the <code>clonal.em.txt</code> file we uploaded earlier, we can use the following command:</p> <pre><code>s3cmd get s3://quadram-bioinfo-training/clonal_heidl/clonal.em.txt\n</code></pre> <p>If we want to download the file to a different name, we can specify this as another parameter. For example, to download the file to <code>new_clonal.em.txt</code>, we can use the following command:</p> <pre><code>s3cmd get s3://quadram-bioinfo-training/clonal_heidl/clonal.em.txt  new_clonal.em.txt\n</code></pre> <p>To download the entire folder back, we can use <code>s3cmd sync</code> in reverse. Note here, that the destination is now the local folder, and the source is now the S3 bucket. For example, to download the entire folder back, we can use the following command:</p> <pre><code>s3cmd sync s3://quadram-bioinfo-training/clonal_heidl/ new_clonal_heidl/\n</code></pre> <p>Which will then start a transfer for the entire folder, to the new location <pre><code>download: 's3://quadram-bioinfo-training/clonal_heidl/clean.full.aln' -&gt; 'new_clonal_heidl/clean.full.aln'  [1 of 8]\n  275906560 of 3546608056     7% in    3s    70.84 MB/s\n..........\n</code></pre></p> <p>You can upload/download data programmatically using <code>Python</code>, see the documentation here.</p>"},{"location":"storage/upload-local-to-s3/","title":"Transferring from your local machine to the CLIMB-BIG-DATA S3 buckets","text":"<p>Transfer between your local machine to the S3 buckets can be done in a number of ways:</p> <ul> <li>Using Bryn's web interface - for Windows, MacOSX and Linux</li> <li>Using a file transfer client (like cyberduck) - for Windows and MacOSX</li> <li>Using the command line - for Windows, MacOSX and Linux</li> </ul>"},{"location":"storage/upload-local-to-s3/#reminder-what-are-the-s3-buckets","title":"Reminder; what are the S3 buckets?","text":"<p>S3 (Simple Storage Service) buckets are cloud storage containers provided by Amazon Web Services (AWS). They allow users to store and retrieve any amount of data securely, making it a scalable and cost-effective solution for storing various types of data, such as files, images, videos, backups, and application data. S3 buckets are highly durable and accessible over the internet, making them a popular choice for hosting static websites, data archiving, and serving as the backend for various cloud applications.</p>"},{"location":"storage/upload-local-to-s3/#using-bryns-web-interface","title":"Using Bryn's web interface","text":"<p>This option is available to all users, via the Bryn web interface. First navigate to the section on S3 buckets, using the side menu:</p> <p></p> <p>You will then see a list of all the S3 buckets for your team. Click on the bucket you want to upload to.</p> <p></p> <p>This will should the contents of the bucket, and there are buttons in the top right to add files or folders to the bucket.</p> <p></p>"},{"location":"storage/upload-local-to-s3/#using-a-file-transfer-client","title":"Using a file transfer client","text":"<p>You can a number of file transfer client software to transfer files to the S3 buckets. You will require API keys to use such clients. These are available in the S3 buckets section of Bryn. Click on the API keys button to reveal your keys.</p> <p></p> <p>To demonstrate, how to transfer files using a file transfer client, we will use Cyberduck. This is available for Windows and MacOSX. Download and install the software, and then open it. You will be presented with a window like this:</p> <p></p> <p>Click on the Open Connection button, and then select Amazon S3 from the dropdown menu. You will then be presented with a window like this:</p> <p></p> <p>Enter the following details:</p> <ul> <li>Server: s3.climb.ac.uk</li> <li>Access Key ID: Your access key ID from Bryn</li> <li>Secret Access Key: Your secret access key from Bryn</li> </ul> <p>Then click Connect. You will then be presented with a window of all the buckets, mirroring that in Bryn. Double click on the bucket you want to upload to, and you will see the contents of the bucket.</p> <p>You can upload to the bucket using the upload button in the top left of the window. You can also drag and drop files and folders into the window to upload them. You can right click on files and folders to download them.</p> <p></p> <p>For more information about Cyberduck, see here.</p>"},{"location":"storage/upload-local-to-s3/#using-s3cmd-on-the-command-line","title":"Using <code>s3cmd</code> on the command line","text":"<p>You can also use the command line to transfer files from your local machine to the S3 buckets. You will require API keys to use the command line. These are available in the S3 buckets section of Bryn. Click on the API keys button to reveal your keys.</p> <p></p> <p>Warning</p> <p>Remember, these are the steps required for you local machine, not the notebook server. The notebook server already has the <code>s3cmd</code> installed and configured.</p> <p>To demonstrate, how to transfer files using the command line, we will use <code>s3cmd</code>. This is available for Windows, Linux and MacOSX. Download and install the software.</p> <p>You can also install <code>s3cmd</code> using pip:</p> <pre><code>pip install s3cmd -U\n</code></pre> <p>Once <code>s3cmd</code> is installed, use the following command to configure it:</p> <pre><code>(s3cmd) ubuntu@eroll:~$ s3cmd --configure\n</code></pre> <p>It will then ask a series of questions. The answers are:</p> <ul> <li>Access Key: Your access key, as displayed in Bryn (see above)</li> <li>Secret Key: Your access key, as displayed in Bryn (see above)</li> <li>S3 Endpoint: s3.climb.ac.uk</li> <li>DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.s3.climb.ac.uk</li> </ul> <p>All other options just use the default values, listed in the square brackets [ ]. You can just press enter to accept the default values.</p> <p>If we use <code>s3cmd</code> to list the buckets, we can see the buckets for our team:</p> <pre><code>(s3cmd) ubuntu@eroll:~$ s3cmd ls\n2023-07-21 08:50  s3://quadram-bioinfo-training\n</code></pre> <p>Once this is configured correctly, <code>s3cmd</code> will behave the same as running it on the notebook server. There is more information on how to use <code>s3cmd</code> here.</p> <p>This is the full configuration process, for reference.</p> <pre><code>(s3cmd) ubuntu@eroll:~$ s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variab\nAccess Key: XXXXXXXXXXXXX\nSecret Key: XXXXXXXXXXXXX\nDefault Region [US]:\n\nUse \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.\nS3 Endpoint [s3.amazonaws.com]: s3.climb.ac.uk\n\nUse \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be\nif the target S3 system supports dns based buckets.\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.s3.climb.ac.uk\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program [/usr/bin/gpg]:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]:\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: XXXXXXXXXX\n  Secret Key: XXXXXXXXXX\n  Default Region: US\n  S3 Endpoint: s3.climb.ac.uk\n  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.s3.climb.ac.uk\n  Encryption password:\n  Path to GPG program: /usr/bin/gpg\n  Use HTTPS protocol: True\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n\nTest access with supplied credentials? [Y/n]\nPlease wait, attempting to list all buckets...\nSuccess. Your access key and secret key worked fine :-)\n\nNow verifying that encryption works...\nNot configured. Never mind.\n\nSave settings? [y/N] Y\nConfiguration saved to '/home/ubuntu/.s3cfg'\n</code></pre>"},{"location":"storage/upload-to-notebook/","title":"Transferring from your local machine to and from your CLIMB-BIG-DATA notebook server","text":"<p>Transfer between your local machine to the notebook server can be done in a number of ways:</p> <ul> <li>Using the file upload in the browser - for Windows, MacOSX and Linux</li> <li>Using the command line - for Linux and MacOSX</li> </ul>"},{"location":"storage/upload-to-notebook/#using-the-file-upload-in-the-browser","title":"Using the file upload in the browser","text":"<p>The file browser and File menu in JupyterLab allow you to manage files and directories on their system. You can perform various actions, such as opening, creating, deleting, renaming, downloading, copying, and sharing files and directories. The file browser is accessible from the left sidebar under the Files tab.</p> <p></p> <p>In the File menu, you can find options like \"New\" and \"Save All\" to create new files and save changes to multiple files simultaneously.</p> <p>To open a file, you can double-click on its name in the file browser, and it will open in the default viewer/editor. Alternatively, they can drag a file into the main work area to create a new tab.</p> <p>JupyterLab supports multiple viewers/editors for various file types. For example, a Markdown file can be opened either in a text editor or as rendered HTML. Additionally, JupyterLab extensions can add new viewers/editors for specific file types. To open a file using a non-default viewer/editor, you can right-click on its name in the file browser and select the desired viewer/editor from the \"Open With\u2026\" submenu.</p> <p>There are complete step-by-step instructions on how to upload files to JupyterLab here.</p>"},{"location":"storage/upload-to-notebook/#using-the-command-line","title":"Using the command line","text":"<p>You can use certain Linux commands copy between your local machine and your notebook server. You must do this from within your notebook server. This can be done with tools such as <code>scp</code> and <code>rsync</code>.</p> <p>From within notebook server terminal, I can use <code>scp</code> to copy files from my local machine to the notebook server. For example, to copy a file called <code>some_file</code> from my local machine to the notebook server, I would use the following command:</p> <pre><code>jovyan:$ scp nabil@101.123.112.1:some_file .\n</code></pre> <p>Where <code>101.123.112.1</code> is the IP address of my local machine. The <code>.</code> at the end of the command means that the file will be copied to the current directory. If I wanted to copy the file to a different directory, I would specify the path to that directory instead of <code>.</code>.</p> <p>Warning</p> <p>This requires access to a local machine (i.e. your local machine should be visible to CLIMB-BIG-DATA) and your local machine has SSH enabled.</p> <p>For more information about <code>scp</code> in general please read the page about <code>scp</code>.</p>"},{"location":"walkthroughs/metagenomics-tutorial/","title":"CLIMB-BIG-DATA: Metagenomics in Brum","text":"<p>In this tutorial we will look at some metagenomics sequences that were sequenced from DNA extracted from the Worcester and Birmingham Canal at the University of Birmingham. In this case the canal water was fetched, DNA extracted and data was generated on the Oxford Nanopore MinION sequencing platform by Josh Quick.</p> <p></p> <p>The Worcester and Birmingham Canal near University Station (photo by Philip Halling)</p>"},{"location":"walkthroughs/metagenomics-tutorial/#before-you-begin","title":"Before you begin","text":"<p>This tutorial assumes you are working on CLIMB-BIG-DATA in a JupyterHub notebook. That is why many of the commands are prepended with the \"!\" (exclamation mark) symbol - which tells the notebook to run the command as if it was on the command line.</p> <p>You could do the tutorial without JupyterHub notebooks in the Terminal, in which case remove the \"!\" before each command.</p> <p>Have a look around this documentation site if you would like to understand more about what a JupyterHub notebook is.</p>"},{"location":"walkthroughs/metagenomics-tutorial/#setting-up-the-environment","title":"Setting up the environment","text":"<p>When working on CLIMB-BIG-DATA with Conda, always work in a new Conda environment. You will not be able to add software to the base environment.</p> <p>We speed the process along by using <code>mamba</code>, a drop-in replacement for <code>conda</code> to create a new environment.</p> <p>Our environment will be called <code>metagenomics-tutorial</code>.</p> <p>In order for this tutorial to function correctly within JupyterHub, also install <code>ipykernel</code>.</p> <pre><code>!mamba create -y -n metagenomics-tutorial ipykernel\n</code></pre> <p>Next, we will need the following software:</p> <ul> <li><code>kraken2</code> - a taxonomic profiler for metagenomics data</li> <li><code>krona</code> - an interactive visualiser for the output of Kraken2</li> <li><code>krakentools</code> - some useful scripts for manipulating Kraken2 output</li> <li><code>taxpasta</code> - a useful tool for converting and merging Kraken2 outputs into other formats like Excel</li> </ul> <pre><code>!mamba install -y kraken2 krona blast krakentools \n</code></pre> <p>Krona reminds us to run <code>ktUpdateTaxonomy.sh</code> before it can be used.</p> <pre><code>!ktUpdateTaxonomy.sh\n</code></pre> <p>Let's grab the CanalSeq data.</p> <p>This is available from the following link.</p> <p>Note that this link is served via the CLIMB S3 service. You can serve your own public data the same way simply by creating a public S3 bucket and uploading files to it!</p>"},{"location":"walkthroughs/metagenomics-tutorial/#running-kraken2","title":"Running Kraken2","text":"<p>You can test <code>kraken2</code> was installed correctly by running it with no command-line options.</p> <pre><code>!kraken2\n</code></pre> <p>Pre-computed Kraken2 databases are available on the <code>/shared/public</code> file system within CLIMB-BIG-DATA. These databases are downloaded from Ben Langmad's publicly available Kraken2 indexes page. These datasets are updated regularly and we will keep all versions that we download available permenantly. Within each database directory <code>latest</code> will always point towards the latest version available.</p> <p>The <code>/shared/public</code> area is designed to store frequently used, important databases for the microbial genomics community. We are just getting started building this resource so please contact us with suggestions for other databases you would like to see here.</p> <p>We can take a look at the latest versions of the databases that are available, and their sizes:</p> <pre><code>!du -h /shared/public/db/kraken2/*/latest/\n</code></pre> <p>11G     /shared/public/db/kraken2/eupathDB46/latest/ 9.6G    /shared/public/db/kraken2/minusb/latest/ 743G    /shared/public/db/kraken2/nt/latest/ 15G     /shared/public/db/kraken2/pluspf_16gb/latest/ 7.6G    /shared/public/db/kraken2/pluspf_8gb/latest/ 75G     /shared/public/db/kraken2/pluspf/latest/ 16G     /shared/public/db/kraken2/pluspfp_16gb/latest/ 7.9G    /shared/public/db/kraken2/pluspfp_8gb/latest/ 169G    /shared/public/db/kraken2/pluspfp/latest/ 15G     /shared/public/db/kraken2/standard_16gb/latest/ 7.6G    /shared/public/db/kraken2/standard_8gb/latest/ 70G     /shared/public/db/kraken2/standard/latest/ 633M    /shared/public/db/kraken2/viral/latest/</p> <p>We can run Kraken2 directly within this JupyterHub notebook which is running in a container. A standard container has 8 CPu cores and 64Gb of memory. Kraken2 doesn't run well unless the database fits into memory, so we can use one of the smaller databases for now such as <code>standard_16gb</code> which contains archaea, bacteria, viral, plasmid, human and UniVec_Core sequences from RefSeq, but subsampled down to a 16Gb database. This will be fast, but we trade off specificity and sensitivity against bigger databases.</p> <pre><code>!kraken2 --threads 8 \\\n   --db /shared/public/db/kraken2/standard_16gb/latest \\\n   --output canalseq.hits.txt \\\n   --report canalseq.report.txt \\\n   canalseq.fasta\n</code></pre> <pre><code>Loading database information... done.\n37407 sequences (91.32 Mbp) processed in 4.876s (460.3 Kseq/m, 1123.76 Mbp/m).\n  12486 sequences classified (33.38%)\n  24921 sequences unclassified (66.62%)\n</code></pre> <p>About a third of sequences were classified and two-thirds were not.</p> <p>The <code>canalseq.report.txt</code> gives a human-readable output from Kraken2.</p> <pre><code>!cat canalseq.report.txt\n</code></pre> <p>People worry about getting Leptospirosis if they swim in the canal. Any evidence of Leptospira sp. in these results?</p> <pre><code>!grep Leptospira canalseq.report.txt\n</code></pre> <pre><code>  0.03  10  0   O   1643688           Leptospirales\n  0.03  10  0   F   170             Leptospiraceae\n  0.02  9   2   G   171               Leptospira\n  0.01  2   2   S   173                 Leptospira interrogans\n  0.00  1   1   S   28182                   Leptospira noguchii\n  0.00  1   1   S   28183                   Leptospira santarosai\n  0.00  1   1   S   1917830                 Leptospira kobayashii\n  0.00  1   1   S   2564040                 Leptospira tipperaryensis\n  0.00  1   0   G1  2633828                 unclassified Leptospira\n  0.00  1   1   S   1513297                   Leptospira sp. GIMC2001\n</code></pre>"},{"location":"walkthroughs/metagenomics-tutorial/#eek","title":"Eek!","text":"<p>It's easier to look at Kraken2 results visually using a Krona plot:</p> <pre><code>!ktImportTaxonomy -t 5 -m 3 canalseq.report.txt -o KronaReport.html\n</code></pre> <pre><code>   [ WARNING ]  Score column already in use; not reading scores.\nLoading taxonomy...\nImporting canalseq.report.txt...\nWriting KronaReport.html...\n</code></pre> <p>We can look at the Krona report directly within the browser by using the file navigator to the left - open up the KronaReport.html within the <code>shared-team</code> directory where we are working. Click around the Krona report to see what is in there.</p> <p>With the <code>extract_kraken_reads.py</code> script in <code>krakentools</code> we can quite easily extract a set of reads that we are interested in for further exploration: perhaps to use a more specific method like BLAST against a large protein database, or to extract for de novo assembly.</p> <pre><code>!extract_kraken_reads.py -k canalseq.hits.txt -s canalseq.fasta -r canalseq.report.txt -t 171 -o leptospira.fasta --include-children\n</code></pre> <pre><code>!extract_kraken_reads.py -k canalseq.hits.txt -s canalseq.fasta -r canalseq.report.txt -t 173 -o linterrogens.fasta --include-children\n</code></pre> <pre><code>PROGRAM START TIME: 04-16-2023 18:40:00\n&gt;&gt; STEP 0: PARSING REPORT FILE canalseq.report.txt\n    1 taxonomy IDs to parse\n&gt;&gt; STEP 1: PARSING KRAKEN FILE FOR READIDS canalseq.hits.txt\n    0.04 million reads processed\n    2 read IDs saved\n&gt;&gt; STEP 2: READING SEQUENCE FILES AND WRITING READS\n    2 read IDs found (0.02 mill reads processed)\n    2 reads printed to file\n    Generated file: linterrogens.fasta\nPROGRAM END TIME: 04-16-2023 18:40:00\n</code></pre> <pre><code>!cat leptospira.fasta\n</code></pre> <p>If you wished you could go and take these reads and BLAST them over at NCBI-BLAST. There is also the <code>nr</code> BLAST database available on CLIMB-BIG-DATA if you wanted to run <code>blastx</code> on them. The BLAST databases are found in <code>/shared/public/db/blast</code>.</p> <p>It was a bit disappointing that only 33% of the reads in our dataset were assigned. We could try a much bigger database than <code>standard_16gb</code> such as <code>pluspfp</code> which contains protozoal, fungal and plant sequences, as well as taxa contained in <code>standard</code>. </p> <p>To do this we will need to use the Kubernetes cluster in CLIMB-BIG-DATA. With the Kubernetes cluster we can run much bigger tasks requiring much more CPU power, or more memory than in a notebook container like this.</p> <p>The easiest way to get started with Kubernetes is using Nextflow.</p> <p>When you run a Nextflow script it will automagically use Kubernetes.</p> <p>There are a few Kraken2 Nextflow scripts, the simplest one I have found is metashot/kraken2. This will also do a few extra steps like running Bracken.</p> <p>If you wanted to run Kraken2 through Nextflow the same way as before you could run:</p> <pre><code>!nextflow run metashot/kraken2 \\\n  -c /etc/nextflow.config \\\n  --reads canalseq.fastq \\\n  --kraken2_db /shared/public/db/kraken2/standard_16gb/latest \\\n  --read_len 100 \\\n  --outdir canalseq-standard16 \\\n  --single_end\n</code></pre> <p>But - and for our final trick - we would like to use a much bigger database <code>pluspfp</code>. We can just re-check it's size.</p> <pre><code>!du -h -d1 /shared/public/db/kraken2/pluspfp/latest\n</code></pre> <p>As this database is around 145Gb, we can ask Nextflow to give us 200 gigabytes of RAM when running this container, to ensure this database fits easily in memory. We could also ask for a lot more CPUs to speed things along further! Nextflow and Kubernetes will take care of finding a machine the best size for this workflow.</p> <pre><code>!nextflow run metashot/kraken2 \\\n  -c /etc/nextflow.config \\\n  --reads canalseq.fasta \\\n  --kraken2_db /shared/public/db/kraken2/pluspfp/latest \\\n  --read_len 100 \\\n  --outdir canalseq-pluspfp \\\n  --single_end \\\n  --max_memory 200.G \\\n  --max_cpus 64\n</code></pre> <p>Ah, OK this gives 80% assignment! We can go and take a look at this in Krona again.</p> <pre><code>!ktImportTaxonomy -t 5 -m 3 -o krona-canalseq-pluspfp.html canalseq-pluspfp/kraken2/canalseq.kraken2.report\n</code></pre> <pre><code>   [ WARNING ]  Score column already in use; not reading scores.\nLoading taxonomy...\nImporting canalseq-pluspfp/kraken2/canalseq.kraken2.report...\n   [ WARNING ]  The following taxonomy IDs were not found in the local\n                database and were set to root (if they were recently added to\n                NCBI, use updateTaxonomy.sh to update the local database):\n                42857\nWriting krona-canalseq-pluspfp.html...\n</code></pre> <p>Lots more taxa to explore if you open up <code>krona-canalseq-pluspfp.htm</code> in the browser on the left!</p>"},{"location":"walkthroughs/metagenomics-tutorial/#r-and-rstudio","title":"R and RStudio","text":"<p>Let's finish up by comparing the results of Kraken2 using the standard-16 database versus the full-fat PlusPFP database!</p> <p>We can use a nice little tool called <code>taxpasta</code> to take the results of the two Kraken2 runs, merge them together and write out a tabular format file that will load easily into R.</p> <pre><code>!cp canalseq-pluspfp/kraken2/canalseq.kraken2.report pluspfp.report\n!cp canalseq-standard16/kraken2/canalseq.kraken2.report standard16.report\n!taxpasta merge --profiler kraken2  --output-format tsv --add-name --add-rank --taxonomy /shared/public/db/taxonomy/latest -o canalseq.merged.tsv pluspfp.report standard16.report\n</code></pre> <p>Now we can do some magic with R.</p> <p>For the next part, either change the running kernel type (using the dropdown menu on the top right) to \"R\", or flip over to RStudio (via File - New Launcher in the menu bar).</p> <pre><code>library(tidyverse)\n</code></pre> <pre><code>UsageError: Cell magic `%%R` not found.\n</code></pre> <pre><code>df = read_tsv(\"canalseq.merged.tsv\", show_col_types=F)\n</code></pre> <pre><code>head(df)\n</code></pre> A tibble: 6 \u00d7 5 taxonomy_idnamerankpluspfpstandard16 &lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;      0NA                NA          775124921      1root              no rank       45   20 131567cellular organismsno rank     2167  381   2759Eukaryota         superkingdom 742    1  33090Viridiplantae     kingdom       12    0  35493Streptophyta      phylum         0    0 <p>A bit of <code>tidyverse</code> magic can plot us the top 20 genera for each of the Kraken2 databases used side by side.</p> <pre><code>df %&gt;% \n    gather(key=\"db\", val=\"count\", 4:5) %&gt;%\n    filter(rank=='genus') %&gt;%\n    slice_max(n=20, count, by=\"db\") %&gt;%\n    ggplot(., aes(x=name, y=count, fill=db)) +\n    geom_bar(stat='identity', position=\"dodge\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n</code></pre> <p></p> <p>And that's the end of the tutorial. </p>"},{"location":"walkthroughs/nfcore/","title":"Running nf-core pipelines","text":""},{"location":"walkthroughs/nfcore/#what-are-nf-core-pipelines","title":"What are nf-core pipelines?","text":"<p>nf-core is an organisation backing an international effort to create high-quality, reproducible pipelines written in Nextflow.</p> <p>Some examples of nf-core pipelines include:</p> <ul> <li>nf-core/fetchngs: to download raw datasets from public repositories (ENA, SRA...)</li> <li>nf-core/rnaseq: to perform a differential expression analysis of RNA-Seq datasets</li> <li>nf-core/ampliseq: to analyse metabarcoding (16S, ITS...) experiments (mostly based on Qiime2)</li> <li>nf-core/taxprofiler: to run multiple taxonomy profiling tools on a metagenomics dataset</li> <li> <p>nf-core/mag: to assemble and bin whole metagenome sequencing runs</p> </li> <li> <p>See the full list online.</p> </li> <li> <p>\ud83d\udca1 See also Using Nextflow</p> </li> </ul>"},{"location":"walkthroughs/nfcore/#how-to-run-a-nf-core-pipeline","title":"How to run a nf-core pipeline?","text":"<p>There is a very good documentation available from the nf-core website, and  even a great set of video tutorials.</p> <p>A first attempt of running a pipeline should be using its test profile. This means that the pipeline will try to analyse some test data known to work, and after getting a successful ending we can go further and try with our own data.</p> <p>The general syntax is: <pre><code>nextflow run nf-core/&lt;pipeline_name&gt; -r &lt;version&gt; -profile test --outdir  /shared/team/&lt;output-dir&gt;\n</code></pre></p> <p>Where:</p> <ul> <li><code>&lt;pipeline_name&gt;</code> is of course the actual pipeline you want to run</li> <li><code>&lt;version&gt;</code> is the revision you want to use (this is important and will ensure reproducibility, check the pipeline website to see the last version)</li> <li><code>&lt;output-dir&gt;</code> where Nextflow will save the files. NOTE that your home directory will not work!</li> </ul> <p>For example, to test the <code>rnaseq</code> pipeline:</p> <pre><code>nextflow run nf-core/rnaseq -r 3.14.0 -profile test --outdir /shared/team/test-out-rnaseq\n</code></pre>"},{"location":"walkthroughs/nfcore/#an-example-fetchngs","title":"An example: fetchngs","text":"<p><code>nf-core/fetchngs</code> is a pipeline to download a set of NGS output from public repositories such as NCBI Short Reads Archive.</p> <p>We can use it as a first example pipeline as its input is a simple text file with a list of accession codes.</p> <p>Remembering that Nextflow pipelines will not have access to any file saved in your home directory, we can create an input file like:</p> <pre><code>mkdir -p /shared/team/download-lists/\necho -e \"ERR12319563\\nERR12319484\\nERR12319547\" &gt; /shared/team/download-lists/test.csv\n</code></pre> <p>Edit</p> <p>The <code>echo</code> command created a list with three accession numbers from the command line,  but you can use the handy text editor built-in in the CLIMB notebook to create a new file. It's important to use the <code>csv</code> extension though.</p> <pre><code># The \\ in the command allows to break a command in multiple lines\n# If you type the command in a single line, do NOT type the \"\\\"s\n\nnextflow run nf-core/fetchngs -r 1.12.0 \\\n   --input /shared/team/download-lists/test.csv \\\n   --outdir /shared/team/fetchngs-out/\n</code></pre> <p>Example execution:</p> <p></p>"},{"location":"walkthroughs/nfcore/#s3-buckets","title":"S3 buckets","text":"<p>A very handy feature of Nextflow, is that it can read and write to S3 buckets.</p> <p>If we want to save the output of the nf-core/fetchngs pipeline to a CLIMB S3 bucket  (suppose you have a bucket called \"ngs-files\"), we can simply change the output path to something like:</p> <pre><code># The \\ in the command allows to break a command in multiple lines\n# If you type the command in a single line, do NOT type the \"\\\"s\n\nnextflow run nf-core/fetchngs -r 1.12.0 \\\n   --input /shared/team/download-lists/test.csv \\\n   --outdir s3://ngs-files/fetchngs-output/\n</code></pre>"},{"location":"walkthroughs/qiime2/","title":"Using QIIME 2 on Jupyter Notbook Servers","text":""},{"location":"walkthroughs/qiime2/#what-is-qiime-2","title":"What is QIIME 2?","text":"<p>QIIME 2 (Boylan et al., 2019) is a microbiome analysis package with an emphasis on data transparency. In short it allows for end-to-end analysis, including demupltiplexing, noise correction, taxonomic classification, phylogeny inference and plotting. Below is a flow chart of the conceptual outline of QIIME 2 (courtesy of the authors, found here).</p> <p></p>"},{"location":"walkthroughs/qiime2/#installation","title":"Installation","text":""},{"location":"walkthroughs/qiime2/#conda","title":"Conda","text":"<p>To install via Conda first you must download the environment YAML file supplied by the QIIME 2 authors. </p> <p><code>wget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml</code></p> <p>This will download a YAML file called <code>qiime2-2023.7-py38-linux-conda.yml</code>. You can create an environment from this YAML:</p> <p><code>conda env create -n qiime --file qiime2-2023.7-py38-linux-conda.yml</code></p> <p>In some instances the creation of the QIIME 2 Conda environment will not solve. This may be caused by the Conda configuration being set to to having a strict priority. Briefly, Conda channels are locations where the packages are stored. When you install a Conda package you are downloading and installing them from remote URLs. These channels are the warehouse where packages are managed. Different channels can host the same package. This can be to the users' benefit as they can access different management structures for a given package, but it can also be to their detriment as there can be collisions in how these packages are managed. </p> <p>In short, strict channel priority can lead to lower priority channels to not be considered if a package is in a higher priority channel. For more information, see here. As QIIME 2 has a large number of dependencies, this can lead to difficulty installing it.</p> <p>To see if strict priority is enabled run <code>conda config --show | grep channel_priority</code>. If it is enabled then <code>channel_priority: strict</code> will be returned. To rectify this run:</p> <p><code>conda config --set channel_priority flexible</code></p>"},{"location":"walkthroughs/qiime2/#docker","title":"Docker","text":"<p>You do not have to actively build a Docker container of QIIME 2 to run it. The Authors maintain a Docker container hosted at quay.io. This can be useful as Nextflow pipelines can leverage Kubernetes pods on the fly, giving you access to more compute.</p>"},{"location":"walkthroughs/qiime2/#running-qiime-2","title":"Running QIIME 2","text":"<p>QIIME 2 can be ran both from the command line interface (CLI) and directly through Python by importing it as a module. There are some advantages to both. The CLI allows one to work freely through the command line, gives one access to many commands and can have tab-completion enabled. The Python API allows for finer grained controls and can be quicker as intermediate files do not need to be read to and from disk.</p>"},{"location":"walkthroughs/qiime2/#command-line-interface","title":"Command Line Interface","text":"<p>First we need to activate the environment we have built:</p> <p><code>conda activate qiime</code></p> <p>Now we have access to QIIME 2. Download some sample single end data and place it in a directory called <code>se-reads</code>:</p> <pre><code>mkdir se-reads ; wget \\\n  -O \"se-reads/barcodes.fastq.gz\" \\\n  \"https://data.qiime2.org/2023.7/tutorials/moving-pictures/emp-single-end-sequences/barcodes.fastq.gz\"\n</code></pre> <p>then download its corresponding metadata:</p> <pre><code>wget \\\n  -O \"sample-metadata.tsv\" \\\n  \"https://data.qiime2.org/2023.7/tutorials/moving-pictures/sample_metadata.tsv\"\n</code></pre> <p>The reads can then be imported into QIIME 2.  Here, QIIME 2 will convert the fastq file into an artifact which can be interacted with downstream:</p> <pre><code>qiime tools import \\\n  --type EMPSingleEndSequences \\\n  --input-path se-reads \\\n  --output-path se-read.qza\n</code></pre> <p>and then be demultiplexed:</p> <pre><code>qiime demux emp-single \\\n  --i-seqs se-reads \\\n  --m-barcodes-file sample-metadata.tsv \\\n  --m-barcodes-column barcode-sequence \\\n  --o-per-sample-sequences demux.qza \\\n  --o-error-correction-details demux-details.qza\n</code></pre> <p>The QIIME 2 ecosystem is substantial. For a reference of the different functionalities, see here.</p>"},{"location":"walkthroughs/qiime2/#nextflow","title":"Nextflow","text":"<p>The same CLI tools can be incorporated into Nextflow pipelines. There is a pre-existing Nextflow config file that allows any Nextflow process running in a Docker container to be launched using the Kubernetes executor. Please see the \"Using Nextflow\" tutorial for a full walkthrough. A number of functions in QIIME 2 can be parallelized, which serve as great candidates for being launched in Nextflow as they can take full advantage of the Kubernetes pods. Below is an example process to compute a multiple sequence alignment using MAFFT:</p> <pre><code>process mafft {\n    cpus 8\n    memory \"16GB\"\n    container=\"quay.io/qiime2/core\"\n\n    input:\n    path(sequence_qza)\n\n    output:\n    path(alignment.qza), emit: alignment_qza\n\n    script:\n    \"\"\"\n    qiime alignment mafft --i-sequences $sequence_qza --p-n-threads \"${task.cpus}\" --o-alignment alignment\n    \"\"\"\n}\n</code></pre> <p>Note that the container variable is defined using the QIIME 2 authors' Docker container. This allows for a Kubernetes pod to be span up.</p>"},{"location":"walkthroughs/qiime2/#jupyter-notebook-python-api","title":"Jupyter Notebook (Python API)","text":"<p>QIIME 2 can be imported as a Python module. It is interoperable with a number of other Python libraries such as Pandas and Biom.</p> <pre><code>from qiime2 import Artifact\nfrom qiime2.plugins import feature_table\n\nimport pandas as pd\n\n#read in a table of features\nunrarefied_table = Artifact.load('table.qza')\nrarefy_result = feature_table.methods.rarefy(table=unrarefied_table, sampling_depth=100)\nrarefied_table = rarefy_result.rarefied_table\n\n#convert to Pandas DF\ndf = rarefied_table.view(pd.DataFrame)\ndf.head()\n</code></pre>"},{"location":"walkthroughs/genome-assembly/about-genome-assembly/","title":"What is a genome assembler doing?","text":"<p>Genome assemblers assume the following about sequenced reads:</p> <ul> <li>Reads are resolved into nucleotide bases (ATGC &amp; ambiguous base calls)</li> <li>Reads are randomly distributed across the target DNA, and</li> <li>Reads are represent an oversampling of the target DNA, such that individual reads repeatedly overlap</li> <li>Genome assemblers calculate overlaps between reads and (usually) represent as a graph/network. Then \"walk\" the graph to determine the original sequence.</li> </ul> <p>See Torsten Seemann\u2019s slides on de novo genome assembly</p> <p></p> <p>Whole genome shotgun sequencing: Genome is sheared into small approximately equal sized fragments which are subsequently small enough to be sequenced in both directions followed by cloning. The cloned sequences (reads) are then fed to an assembler (illustrated in Figure 2). b To overcome some of the complexity of normal shotgun sequencing of large sequences such as genomes a hierarchical approach can be taken. The genome is broken into a series of large equal segments of known order which are then subject to shotgun sequencing. The assembly process here is simpler and less computationally expensive. From Commins, Toft and Fares 2009</p>"},{"location":"walkthroughs/genome-assembly/about-genome-assembly/#what-is-r1-and-r2","title":"What is R1 and R2?","text":"<p>Just as a reminder, many sequencing library preparation kits include an option to generate so-called \"paired-end reads\".  Intact genomic DNA is sheared into several million short DNA fragment.  Individual reads can be paired together to create paired-end reads, which offers some benefits for downstream bioinformatics data analysis algorithms.  The structure of a paired-end read is shown here.</p> <p></p> <ul> <li>\"Read 1\", often called the \"forward read\", extends from the \"Read 1 Adapter\" in the 5\u2032 \u2013 3\u2032 direction towards \"Read 2\" along the forward DNA strand.</li> <li>\"Read 2\", often called the \"reverse read\", extends from the \"Read 2 Adapter\" in the 5\u2032 \u2013 3\u2032 direction towards \"Read 1\" along the reverse DNA strand.</li> </ul>"},{"location":"walkthroughs/genome-assembly/about-genome-assembly/#how-genome-assemblers-fail-perfection","title":"How genome assemblers fail perfection","text":"<p>In theory, Genome assembly software with perfect reads of good length will reconstruct the genome verbatim </p> <p>However, Sequencing platform have errors (and cause errors downstream): </p> <ul> <li>Struggle with GC rich and/or AT rich DNA.</li> <li>Have lower read quality towards the end of reads (5', 3' or both ends)</li> <li>Have difficulty reading homopolymers (e.g. AAAAA or TTTTTTT) accurately</li> <li>Have read lengths that does not span repeated sequences in the genome</li> </ul> <p></p> <ul> <li>Repeats: A segment of DNA that occurs more than once in the genome</li> <li>Read length must span the repeat</li> </ul>"},{"location":"walkthroughs/genome-assembly/about-genome-assembly/#outcomes-of-your-final-contigs","title":"Outcomes of your final contigs","text":"<p>Mate-pair signatures for collapse style mis-assemblies. (a) Two copy tandem repeat R shown with properly sized and oriented mate-pairs. (b) Collapsed tandem repeat shown with compressed and mis-oriented mate-pairs. (c) Two copy repeat R, bounding unique sequence B, shown with properly sized and oriented mate-pairs. (d) Collapsed repeat shown with compressed and mis-linked mate-pairs. From https://doi.org/10.1186%2Fgb-2008-9-3-r55</p>"},{"location":"walkthroughs/genome-assembly/about-genome-assembly/#how-to-span-repeats","title":"How to span repeats","text":"<ul> <li>Long reads (ONT, Pacbio)</li> <li>Long reads (Sanger)</li> <li>Optical mapping</li> <li>Hi-C</li> <li>Or just don\u2019t! </li> </ul>"},{"location":"walkthroughs/genome-assembly/spades/","title":"Assembling a genome from short reads (e.g. Illumina) using SPAdes","text":"<p>Acknowledgements</p> <p>This walkthrough was adapted from Conor Meehan's Pathogen genomics course under a CC BY-NC-SA 4.0 license. The walkthrough was modified to include CLIMB-BIG-DATA specific intructions and additional explanation of certain steps. </p> <p>De novo genome assembly is a process used to reconstruct the complete genome sequence of an organism without the need for a reference genome. In the case of bacterial genomes, de novo assembly involves piecing together short DNA sequences, called reads, obtained from high-throughput sequencing technologies. In this walkthrough, you will learn how to use <code>SPAdes</code> to assemble short reads into a genome. If you would like to read more about genome assembly, there is more information here.</p> <p>Other tools associated with <code>SPAdes</code> can be used to assemble metagenomes sequenced using short read technologies (<code>metaSPAdes</code>) or viral genomes (e.g. <code>rnaviralSPAdes</code> or <code>coronaSPAdes</code>). This is not covered here but is outlined in the SPAdes github and associated <code>metaSPAdes</code> and <code>coronaSPAdes</code> papers.</p> <p>We will:</p> <ul> <li>Manage our software environment using <code>conda</code></li> <li>Download some sequenced reads into your CLIMB-BIG-DATA session</li> <li>Trim the reads to remove low quality bases and reads with <code>trimmomatic</code></li> <li>Assemble the reads into a genome using <code>SPAdes</code></li> </ul> <p>This demonstration uses one of the samples from Hikichi et al which are the DRR187559_1.fastqsanger.bz2 and DRR187559_2.fastqsanger.bz2 files from this zenodo record</p>"},{"location":"walkthroughs/genome-assembly/spades/#begin","title":"BEGIN","text":""},{"location":"walkthroughs/genome-assembly/spades/#launch-terminal","title":"Launch terminal","text":"<p>Select <code>File -&gt; New -&gt; Terminal</code>, or click the Terminal icon on the launcher pane. You'll get a new terminal tab in the activity bar, and find yourself in a bash shell.</p> <p></p> <p>This will open a new terminal session </p> <p></p>"},{"location":"walkthroughs/genome-assembly/spades/#create-a-folder-to-store-the-sequenced-read-data-and-the-genome-assembly","title":"Create a folder to store the sequenced read data and the genome assembly","text":"<p>Create a folder called <code>spades-demo</code> followed by your name in the <code>shared-team</code> directory, (i.e. <code>spades-demo-rupert</code>) and move into it. This is where we will store the data and the genome assembly.</p> <pre><code>mkdir ~/shared-team/spades-demo-rupert\ncd ~/shared-team/spades-demo-rupert\n</code></pre> <p>Download the fastq raw data into this folder. These are somewhat large files and will take about one minute. <pre><code>wget https://zenodo.org/record/4534098/files/DRR187559_1.fastqsanger.bz2\nwget https://zenodo.org/record/4534098/files/DRR187559_2.fastqsanger.bz2\n</code></pre></p> <p>Info</p> <p>Remember you can copy-paste commands from this walkthrough into the terminal.</p> <p>Trimmomatic and SPAdes prefer gzipped files (files that end in .gz) but our downloaded file is a bzip2 file (ends in .bz2). We need to convert these types before we proceed. We will also rename them into more conventional naming scheme for illumina reads (that is, they end in _1.fastq.gz and _2.fastq.gz)</p> <pre><code>bzcat DRR187559_1.fastqsanger.bz2 | gzip -c &gt; DRR187559_1.fastq.gz\nbzcat DRR187559_2.fastqsanger.bz2 | gzip -c &gt; DRR187559_2.fastq.gz\n</code></pre> <p>Most sequencers produce .gz files so this is not necessary if your file is already in that format.</p> <p>Use <code>ls</code> to check that the files are there and have the correct names <pre><code>ls -ahl \n</code></pre></p> <p>Which should give you something like this: <pre><code>jovyan:~/shared-team/spades-demo-rupert$ ls -ahl \ntotal 272M\ndrwxr-xr-x 2 jovyan users   4 Jul 20 17:17 .\ndrwx------ 7 jovyan  1000   9 Jul 20 17:13 ..\n-rw-r--r-- 1 jovyan users 73M Jul 20 17:17 DRR187559_1.fastq.gz\n-rw-r--r-- 1 jovyan users 61M Jul 13 21:00 DRR187559_1.fastqsanger.bz2\n-rw-r--r-- 1 jovyan users 76M Jul 20 17:17 DRR187559_2.fastq.gz\n-rw-r--r-- 1 jovyan users 63M Jul 13 21:00 DRR187559_2.fastqsanger.bz2\n</code></pre></p>"},{"location":"walkthroughs/genome-assembly/spades/#install-spades-and-trimmomatic-using-conda","title":"Install SPAdes and trimmomatic using Conda","text":"<p>It is recommended to always install packages for a project in their own environments so here will we create an enironment and install SPAdes in one step. <pre><code>conda create --solver libmamba -y -n assembly -c bioconda spades trimmomatic\nconda activate assembly\n</code></pre></p> <p>Warning</p> <p>Remember you can not install packages in the base Conda environment. You must create a new environment and activate it first.</p>"},{"location":"walkthroughs/genome-assembly/spades/#use-trimmomatic-to-filter-out-bad-reads-and-trim-the-ends-as-needed","title":"Use trimmomatic to filter out bad reads and trim the ends as needed","text":"<p>The order of commands for paired end reads is:  <pre><code>trimmomatic PE &lt;forward reads input name&gt; \n               &lt;reverse reads input name&gt; \n               &lt;trimmed forward reads out name&gt; \n               &lt;removed forward reads out name&gt; \n               &lt;trimmed reverse reads out name&gt; \n               &lt;removed reverse reads out name&gt; \n               &lt;trimming options&gt;\n</code></pre></p> <p>We will trim the reads in the follow ways:</p> <ul> <li>Remove all reads shorter than 30bp MINLEN:30</li> <li>Remove bases from the start of a read if they are below a quality score of 20, LEADING:20</li> <li>Remove bases from the END of a read if they are below a quality score of 20, TRAILING:20</li> <li>Use a sliding window of 4 bases across the read, removing any set of 4 bases where the average score drops below 20, SLIDINGWINDOW:4:20</li> </ul> <pre><code>trimmomatic PE DRR187559_1.fastq.gz DRR187559_2.fastq.gz DRR187559_trimmed_1.fastq.gz DRR187559_removed_1.fastq.gz DRR187559_trimmed_2.fastq.gz DRR187559_removed_2.fastq.gz  MINLEN:30 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20\n</code></pre> <p></p> <p>At this stage you should use Fastqc to look at the read quality and see if it is ok, but we will skip this step as it is not always needed.</p>"},{"location":"walkthroughs/genome-assembly/spades/#assemble-the-genome-using-spades","title":"Assemble the genome using SPAdes","text":"<ul> <li>The <code>-1</code> and <code>-2</code> flags are the forward and reverse trimmed reads that came from trimmomatic, respectively.</li> <li><code>-o</code>is the anme of a directory to store the output in</li> <li><code>-t</code> is the number of threads to use. This should be the number of cores you have available.</li> <li><code>-m</code> is the amount of memory that SPAdes can use. This should be the amount of memory you have available.</li> </ul> <pre><code>spades.py -1 DRR187559_trimmed_1.fastq.gz -2 DRR187559_trimmed_2.fastq.gz -o DRR187559_spades -t 6 -m 8\n</code></pre>"},{"location":"walkthroughs/genome-assembly/spades/#examining-the-output","title":"Examining the output","text":"<p>SPAdes will mention all the output files in the log. All the output will be in the folder we specified with the <code>-o</code> flag.</p> <pre><code>===== Terminate finished. \n\n * Corrected reads are in /shared/team/spades-demo-rupert/DRR187559_spades/corrected/\n * Assembled contigs are in /shared/team/spades-demo-rupert/DRR187559_spades/contigs.fasta\n * Assembled scaffolds are in /shared/team/spades-demo-rupert/DRR187559_spades/scaffolds.fasta\n * Paths in the assembly graph corresponding to the contigs are in /shared/team/spades-demo-rupert/DRR187559_spades/contigs.paths\n * Paths in the assembly graph corresponding to the scaffolds are in /shared/team/spades-demo-rupert/DRR187559_spades/scaffolds.paths\n * Assembly graph is in /shared/team/spades-demo-rupert/DRR187559_spades/assembly_graph.fastg\n * Assembly graph in GFA format is in /shared/team/spades-demo-rupert/DRR187559_spades/assembly_graph_with_scaffolds.gfa\n</code></pre> <p>The important output files:</p> <ul> <li>scaffolds.fasta: The final assembly you should use</li> <li>assembly_graph.fastg: Assembly graph </li> <li>spades.log: Full log file for bug reporting</li> </ul> <p>Use the <code>head</code> command to have a look at the final output assembly file:</p> <pre><code>head DRR187559_spades/scaffolds.fasta \n</code></pre> <p>Which should give you something like this: <pre><code>&gt;NODE_1_length_419723_cov_11.898846\nTACAAAGGAGAAAAAAAGAAGACAACCAAGCCCAATAATGGACTGGCCGCCTAATAATAA\nAAACTCTAAAAGTTGTAATTTAAAATAGTTCTTTAAATTATATACCCACCACATTTGGTG\nGAGAACCAAAAATTAGCCGAAAAACATCATTTCTGAAGTTATCGGCTAAAGTTATAAATT\nATATTTATTTGTACATGAACAAATAATTTACATTAATTTGTCATTTCTTCTTTTTCCCAA\nTCGATTTTATATCTTTCTGAAGAACGATCTGTCCATTTATCTTTAGTATTGGTACCTTTC\nCAATTTGTTGAAGTCCAATGCAATTGGTAGTCATCACGAACTCGTTCGTATATTACATCT\nATATTTGTTTGTTGTTTGGATGCTTTTCTATCCATAGTAATAACTGTAGCGAAGTCTGGT\nGAAAACCCTGAAGATAATAGAGAACTTGCTTTGTTAGGATCAAGGAAGTTCTCTGCTGCT\nTTCATAGAACCATTTCTAGTTTTCATGAAAAGTTGATTGCCATATACCGGGTTCCAAGAA\n</code></pre></p> <p>Deactivate your Conda environment when you are finished</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"walkthroughs/genome-assembly/spades/#post-assembly-steps","title":"Post assembly steps","text":"<p>Once assembly is finished you can do a more in depth check of the quality and completeness using the BUSCO and Bandage. You can get basic metrics such as N50 using the tool Quast which can be downloaded via Conda</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/","title":"Creating Custom Nextflow Workflows","text":"<p>Suggested reading before starting this tutorial:</p> <ul> <li>Notebook Servers, Quick Start</li> <li>Using the Terminal</li> <li>Using Nextflow</li> <li>Understanding Storage</li> </ul> <p>Workflow management systems such as Nextflow can be used to create reproducible and scalable workflows. This tutorial consists of taking a test pipeline with the following workflow:</p> <p></p> <p>And modifying and extending it to follow this workflow:</p> <p></p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#task-1-try-running-the-test-pipeline","title":"TASK 1: Try running the test pipeline","text":"<p>Background reading: nextflow docs: basic concepts.</p> <p>This tutorial uses the MRC-CLIMB/modules GitHub repository. The module files in this repo contain the processes which we\u2019ll use to build the workflow.</p> <p>Create a notebook server and start a terminal session. Next, clone the MRC-CLIMB/modules repo and take a look at the module files in the directory <code>modules/modules</code> using <code>cat</code>. E.g. <code>cat shovill.nf</code></p> <pre><code>jovyan:~$ git clone https://github.com/MRC-CLIMB/modules.git\n[...]\njovyan:~$ cd modules/modules/\njovyan:~/modules/modules$ ls\nabricate.nf  centrifuge.nf  fastp.nf  kraken2.nf  mykrobe.nf  prokka.nf  quast.nf  shovill.nf  trimgalore.nf  trimmomatic.nf\njovyan:~/modules/modules$ cat shovill.nf\nprocess shovill {\n    /**\n    * Assemble bacterial isolate genomes (https://github.com/tseemann/shovill)\n    * @input tuple dataset_id, path(forward), path(reverse)\n    * @output shovill_out tuple val(dataset_id), path(\"${dataset_id}.fasta\")\n    */\n\n    tag { dataset_id }\n\n    cpus 2\n\n    memory '8GB'\n\n    container = \"quay.io/climb-big-data/shovill:1.1.0\"\n\n    publishDir \"${params.outputDir}/${task.process.replaceAll(\":\", \"_\")}\", mode: 'copy', pattern: '*.fasta'\n\n    input:\n\n      tuple val(dataset_id), path(forward), path(reverse)\n\n    output:\n\n      tuple val(dataset_id), path(\"${dataset_id}.fasta\"), emit: shovill_out\n      path(\"${dataset_id}.fasta\"), emit: shovill_quast\n\n    script:\n\n      \"\"\"\n      shovill --cpus ${task.cpus} --R1 ${forward} --R2 ${reverse} --minlen 500 --outdir shovill\n      mv shovill/contigs.fa ${dataset_id}.fasta\n      \"\"\"\n</code></pre> <p>You will see the processes contain the following definitions:</p> <ul> <li> <p>Tag: Custom label for a process (makes it easier to identify a task in the Nextflow logs)</p> </li> <li> <p>Cpus: Number of cpus to allocate to a process</p> </li> <li> <p>Memory: Memory allocated to a process</p> </li> <li> <p>Container: Container used to run the process. Pulled from the climb-big-data quay.io repository</p> </li> <li> <p>PublishDir: Declare output files to be published</p> </li> </ul> <p>And the following declarations:</p> <ul> <li> <p>Input: The expected cardinality for the input channel(s)</p> </li> <li> <p>Output: The expected output files from a process, output channels are named using emit</p> </li> <li> <p>Script: The command/script to run</p> </li> </ul> <p>Note that to run a Nextflow pipeline using Kubernetes (k8s): <code>cpu</code>, <code>memory</code> and <code>container</code> must be defined for every process.</p> <p>Nextflow uses input and output channels to pass data and files between processes. These channels define the execution flow of the pipeline. The script string is executed as a Bash script.</p> <p>Navigate to the directory containing the test pipeline <code>modules/test-pipeline</code>. You\u2019ll see two files: the pipeline itself <code>main.nf</code> and a config file <code>nextflow.config</code>.</p> <p>Take a look at the contents of these files using <code>cat</code>. In <code>main.nf</code>, you\u2019ll see a short workflow is defined as in the first figure above, which uses processes from the module files found in <code>modules/modules</code> and <code>modules/test-datasets</code>, imported using <code>include</code>. Channels are used to pass the input and output data between the processes.</p> <pre><code>jovyan:~/modules/modules$ cd ../test-pipeline/\njovyan:~/modules/test-pipeline$ ls\nmain.nf  nextflow.config\njovyan:~/modules/test-pipeline$ cat main.nf \n#!/usr/bin/env nextflow\n\n// enable dsl2\nnextflow.enable.dsl = 2\n\n// import modules\ninclude {tbfastqs} from '../test-datasets/tbfastqs.nf'\ninclude {trimgalore} from '../modules/trimgalore.nf'\ninclude {shovill} from '../modules/shovill.nf'\n\n// define workflow\nworkflow {\n\n  // main workflow\n  main:\n\n    tbfastqs()\n    trimgalore(tbfastqs.out.tbfastqs_out)\n    shovill(trimgalore.out.trimgalore_out)\n\n}\n</code></pre> <p>In <code>nextflow.config</code>, you\u2019ll see a parameter for the output results directory <code>outputDir</code> needs to be defined.</p> <pre><code>jovyan:~/modules/test-pipeline$ cat nextflow.config \nparams {\n\n  outputDir = \"\"\n\n}\n</code></pre> <p>Open the <code>nextflow.config</code> in a text editor e.g. <code>nano nextflow.config</code> and add a value for <code>outputDir</code>  e.g. <code>/shared/team/modules-out</code>. Your output directory should be within <code>/shared/team</code> as your home directory only has 20GB of storage (Understanding Storage).</p> <p>Now, try running the pipeline:</p> <pre><code>jovyan:~/modules/test-pipeline$ nextflow run main.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `main.nf` [exotic_davinci] DSL2 - revision: ad3ec637d6\nexecutor &gt;  local (1), k8s (2)\n[56/e6bb7a] process &gt; tbfastqs                [100%] 1 of 1 \u2714\n[4e/f93925] process &gt; trimgalore (SRR9588019) [100%] 1 of 1 \u2714\n[a5/0b0c04] process &gt; shovill (SRR9588019)    [100%] 1 of 1 \u2714\nCompleted at: 31-Jul-2023 15:24:21\nDuration    : 10m 27s\nCPU hours   : 0.3\nSucceeded   : 3\n</code></pre> <p>Notice that one process ran locally, and two processes with the k8s <code>executor &gt;  local (1), k8s (2)</code>. If you take a look at the process <code>tbfastqs</code> within <code>modules/test-datasets/tbfastqs.nf</code>, you'll see the directive <code>executor 'local'</code> has been defined, telling Nextflow to run this process locally. By default, processes will run on the k8s unless otherwise specified (due to the defaults set in the CLIMB Nextflow config, you can see what the config looks like using the command <code>nextflow config</code>).</p> <p>Open a new terminal window, if you run <code>ls /shared/team/nxf_work/$JUPYTERHUB_USER</code>, you\u2019ll see a <code>work</code> directory has been created (this is the directory Nextflow uses when running the processes). An output directory with the results from the pipeline will also be created at the path you set in the <code>nextflow.config</code> (the <code>publishDir</code> declaration in a process identifies which output files from a process should be copied from the <code>work</code> directory to the <code>output</code> directory).</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#task-2-remove-tbfastqs-process-and-create-a-channel-for-fastqs","title":"TASK 2: Remove tbfastqs process and create a channel for fastqs","text":"<p>Background reading: nextflow docs: fromFilePairs channel factory</p> <p>The test-pipeline pulls a pair of fastqs from the ENA for testing purposes. In the real world, we would want to pass a directory containing fastq files to the workflow.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-1-remove-tbfastqs-process-from-mainnf","title":"Step 1: Remove tbfastqs process from main.nf","text":"<p>Open the <code>modules/test-pipeline/main.nf</code> file in a text editor, e.g. <code>nano main.nf</code>. Remove the <code>include</code> statement for tbfastqs and remove the tbfastqs process from the workflow itself. </p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-2-add-a-parameter-to-the-nextflowconfig-for-the-input-fastq-directory","title":"Step 2: Add a parameter to the <code>nextflow.config</code> for the input fastq directory","text":"<p>First, let's download a pair of fastqs from the ENA to a directory within <code>/shared/team/</code>, e.g. <code>/shared/team/test-fastqs</code>. We are using <code>/shared/team/</code> as it is mounted to the Kubernetes pods.</p> <pre><code>jovyan:~$ cd /shared/team/\njovyan:/shared/team$ ls\nconda  nxf_work  results  Sting_ec  string\njovyan:/shared/team$ mkdir test-fastqs\njovyan:/shared/team$ cd test-fastqs\njovyan:/shared/team/test-fastqs$ wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR958/009/SRR9588019/SRR9588019_1.fastq.gz\n[...]\njovyan:/shared/team/test-fastqs$ wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR958/009/SRR9588019/SRR9588019_2.fastq.gz\n[...]\njovyan:/shared/team/test-fastqs$ ls\nSRR9588019_1.fastq.gz  SRR9588019_2.fastq.gz\n</code></pre> <p>We want to add this directory with the fastqs as a parameter to the <code>nextflow.config</code> in the <code>params{}</code> declaration. The parameter will take the general form: <pre><code>PARAM_NAME=/PATH/GLOB_FOR_FASTQS\n</code></pre> where</p> <ul> <li> <p>PARAM_NAME: Set a name for the parameter.</p> </li> <li> <p>PATH: Path to the fastqs.</p> </li> <li> <p>GLOB_FOR_FASTQS: We need to set a glob pattern for Nextflow to identify the fastqs pairs. Hint: Take a look at the example in https://www.nextflow.io/docs/latest/channel.html#fromfilepairs</p> </li> </ul>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-3-add-a-channel-for-the-input-fastqs","title":"Step 3: Add a Channel for the input fastqs","text":"<p>In <code>main.nf</code>, add a channel for the input fastqs. In Nextflow, data is passed to processes using channels. An input channel for paired fastqs takes the following general form: <pre><code>Channel.fromFilePairs(INPUT_PATH, OPTIONS)\n       .set{ CHANNEL_NAME }\n</code></pre> where</p> <ul> <li> <p>CHANNEL_NAME: Set a name for the channel.</p> </li> <li> <p>INPUT_PATH: This should be the parameter you set in the config in Step 2. To call a parameter in the main script we use <code>\"${params.PARAM_NAME}\"</code>, substituting in the parameter name you set in Step 2.   </p> </li> <li> <p>OPTIONS: Add in any options. For this workflow, we want to add in the option <code>flat: true</code>, to match the cardinality set in the input declaration for the trimgalore module <code>tuple val(dataset_id), path(forward), path(reverse)</code></p> </li> </ul>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-4-change-the-input-channel-for-trimgalore","title":"Step 4: Change the input channel for trimgalore","text":"<p>The input channel for the trimgalore process is set to the output of the tbfastq process, e.g. <code>trimgalore(tbfastqs.out.tbfastqs_out)</code>. This now needs to be changed to the <code>CHANNEL_NAME</code> you set in Step 3.</p> <p>Now, try running the pipeline again, but this time run the pipeline in the background using the <code>-bg</code> option and redirect the <code>STDOUT</code> to a file: <pre><code>jovyan:~/modules/test-pipeline$ nextflow run main.nf -bg &gt; task2.txt\n</code></pre> You check the progress of the pipeline with <code>cat task2.txt</code></p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#task-3-add-abricate-and-quast-processes","title":"TASK 3:  Add abricate and quast processes","text":"<p>Background reading: nextflow docs: collect operator</p> <p>Take a look at the structure of the current workflow, and try to add the processes from <code>modules/abricate.nf</code> and <code>modules/quast.nf</code> to the workflow.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-1-add-abricate-process-to-the-workflow-remember-to-add-include-statement","title":"Step 1: Add abricate process to the workflow (remember to add include statement!)","text":"<p>A process declaration takes the general form: <pre><code>PROCESS_NAME(INPUT_CHANNEL_1, INPUT_CHANNEL_2, \u2026, INPUT_CHANNEL_N)\n</code></pre> where</p> <ul> <li> <p>PROCESS_NAME: Name of the process as named in the module file.</p> </li> <li> <p>INPUT_CHANNEL_{1:N}: Input channels to the process separated by commas.</p> </li> </ul> <p>E.g. <code>shovill(trimgalore.out.trimgalore_out)</code> uses the output channel <code>trimgalore_out</code> from trimgalore (as defined in the process in <code>modules/trimgalore.nf</code>) for its input channel. For abricate, we have the one input channel which is the output channel from shovill <code>shovill_out</code></p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-2-add-quast-process-to-workflow","title":"Step 2: Add quast process to workflow","text":"<p>Next, try to add the process from <code>modules/quast.nf</code> to the workflow. Quast takes the output channel <code>shovill_quast</code> from shovill as its input channel. However, this time we also need to use an operator on the channel. In the process declaration, operators are used like so: <pre><code>PROCESS_NAME(INPUT_CHANNEL.OPERATOR)\n</code></pre> If we look at the script for the quast process in <code>modules/quast.nf</code> <pre><code>script:\n\n      \"\"\"\n      quast.py -t ${task.cpus} -o . --contig-thresholds 0 --no-html --no-plots *.fasta\n      \"\"\"\n</code></pre></p> <p>you\u2019ll see from the <code>*.fasta</code> wildcard that quast can run on multiple fastas at once. As such, we want to gather the output fastas from all our run shovill processes (note in this example, the shovill process will run only once as we have one pair of fastq to assemble, but if we had multiple pairs of fastqs the shovill process would run multiple times). To do this we will use the operator <code>.collect()</code>.</p> <p>Try running the pipeline again, to see if you\u2019ve correctly added abricate and quast to the workflow.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#task-4-create-channel-for-kraken-2-database-and-add-kraken-2-process","title":"TASK 4: Create channel for Kraken 2 database and add Kraken 2 process","text":"<p>Background reading:</p> <ol> <li>Nextflow docs: fromPath channel factory</li> <li>Nextflow docs: tolist operator</li> </ol> <p>Kraken 2 requires as input a reference database in order to assign species IDs to a sample. We will need to create an input channel for this database.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-1-add-a-parameter-for-the-path-to-the-kraken-2-database","title":"Step 1: Add a parameter for the path to the Kraken 2 database","text":"<p>Add a new parameter to the <code>nextflow.config</code> for the Kraken 2 database path</p> <p>Kraken 2 databases can be found at <code>/shared/public/db/kraken2</code>. We will use the <code>k2_pluspfp_16gb</code> database. The Kraken 2 database files have the extension <code>.k2d</code>. Use this information to construct a parameter for the Kraken 2 database path.</p> <p>Note that <code>/shared/public</code> is also mounted to the Kubenetes pods.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-2-create-a-channel-for-the-database","title":"Step 2: Create a channel for the database","text":"<p>In <code>main.nf</code> create a channel for the Kraken 2 database files. This time use the <code>Channel.fromPath</code> directive, using the parameter you set in Step 1 as the path.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#step-3-add-the-kraken-2-process-to-the-workflow","title":"Step 3: Add the Kraken 2 process to the workflow","text":"<p>Add the Kraken 2 process defined in <code>modules/kraken2.nf</code> to the main workflow. Kraken 2 will take 2 input channels: the output from trimgalore <code>trimgalore_out</code>, and the Kraken 2 database channel from Step 2. Use the operator <code>.toList()</code> for the Kraken 2 database channel (this will emit the Kraken 2 database files as a single item).</p> <p>Once you've completed the above steps, try running the workflow again. Your workflow should now resemble the second figure at the top of this page.</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#solution","title":"Solution","text":"<p>Once you have completed all the tasks, your workflow and config should look something like the following:</p>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#mainnf","title":"<code>main.nf</code>","text":"<pre><code>#!/usr/bin/env nextflow\n\n// enable dsl2\nnextflow.enable.dsl = 2\n\n// import modules\ninclude {trimgalore} from '../modules/trimgalore.nf'\ninclude {shovill} from '../modules/shovill.nf'\ninclude {abricate} from '../modules/abricate.nf'\ninclude {quast} from '../modules/quast.nf'\ninclude {kraken2} from '../modules/kraken2.nf'\n\n// define channels\nChannel.fromFilePairs(\"${params.reads}\", flat: true)\n       .set{ inputFastq }\n\nChannel.fromPath( \"${params.kraken_db}\" )\n       .set{ kraken2DB }\n\n// define workflow\nworkflow {\n\n  // main workflow\n  main:\n\n    trimgalore(inputFastq)\n\n    shovill(trimgalore.out.trimgalore_out)\n\n    abricate(shovill.out.shovill_out)\n\n    quast(shovill.out.shovill_quast.collect())\n\n    kraken2(trimgalore.out.trimgalore_out, kraken2DB.toList())\n\n}\n</code></pre>"},{"location":"walkthroughs/nextflow-custom-workflows/nextflow-custom/#nextflowconfig","title":"<code>nextflow.config</code>","text":"<pre><code>params {\n\n  outputDir = \"/shared/team/modules-out\"\n\n  reads = \"/shared/team/test-fastqs/*_{1,2}.fastq.gz\"\n\n  kraken_db = \"/shared/public/db/kraken2/k2_pluspfp_16gb/*.k2d\"\n\n}\n</code></pre>"}]}